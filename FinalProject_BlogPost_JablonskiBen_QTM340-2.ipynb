{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiating the Humor of Long-Running Animated Adult Comedy TV Shows\n",
    "#### Ben Jablonski\n",
    "#### 11-28-2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Comedy has been a perennial genre in television since the emergence of network television, but there is an incredible variety between the writing and themes of different TV shows. Specifically, animated adult comedy shows have become increasingly popular since the emergence of *The Simpsons* in the early 1990s. The creation of Adult Swim in 2001 created a dedicated time and network for adult animation, and the emergence of streaming services like Netflix and Hulu have cemented funding for new animation projects. Over the last decade, shows like *Rick and Morty* have become cultural sensations. For a more thorough investigation of the history of adult animation, this article from [*Time*](https://time.com/5752400/adult-animation-golden-age/) is a fantastic resource. I wanted to test whether computational techniques such as classification and word2vec models can differentiate between the writing and themes of various animated comedy TV shows. Specifically, which words, themes and relationships between characters define and separate the TV shows from each other? Potentially, these methods can reveal subtextual themes that may not be clear to a casual viewer of the TV show and could elucidate choices made by the writers of TV shows whether the choices are conscious or subconscious. Because of the increasing popularity of animated comedy shows, differentiating and understanding their writing and themes serves an important cultural role through complementing the experience of watching a popular TV show. Specifically, these computational techniques allow a human viewer to further investigate previously banal relationships, and exposing these relationships can provide a critical acuity that enables a richer understanding of a TV show. \n",
    "\n",
    "\n",
    "### Relevant Studies \n",
    "There are two types of studies that are relevant to this analysis. Some studies are data based while others are more anecdotal and focus on philosophical themes of TV shows. For example, for the TV show *South Park*, there are 45 podcast episodes and 12 YouTube videos on the YouTube channel [Wisecrack](https://www.youtube.com/channel/UC6-ymYjG0SU0jUWnWh9ZzEQ) alone. These analyses fall into the second category with most of the crew of Wisecrack being philosophy graduate students or former philosophy professors. One example is the podcast episode on the \"Band in China\" *South Park* episode which discusses international copyright issues and censorship as financial blackmail from China. There are various other studies on YouTube or sites like Medium, but most of these studies focus less on the genre of animated comedy shows as a whole. Instead, they focus on individual episodes or individual TV shows. \n",
    "The other category of studies is more data science based. These studies focus on individual TV shows with the scripts of every episode. Some examples include \"[Going Down to South Park -- Text Analysis with R](https://medium.com/@vertabeloacdm/going-down-to-south-park-text-analysis-with-r-61e8beef6851)\", \"[The Simpsons meets Data Visualization](https://towardsdatascience.com/the-simpsons-meets-data-visualization-ef8ef0819d13)\", \"[Visualizing Archer](https://medium.com/@Elijah_Meeks/visualizing-archer-bcb80e319625)\", or \"[Futurama: Bender's NLP](https://towardsdatascience.com/futarama-benders-nlp-775c47871ad5)\". However, many of these studies count individual character's lines and compare the character's lines between seasons. For example, *The Simpsons* study above finds that Homer and Marge talk to each other the most among every character, and they find that Flanders's lines have the most positive sentiment. Neither of these conclusions provide particular insight into their characters or the themes of the TV show. Additionally, I could not find studies that compare different animated adult comedy TV shows.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "It is difficult to be unbiased in deciding which TV shows to include in the analysis, but only a dozen animated adult comedy shows have run for more than 5 seasons. I wanted to select TV shows with at least 60 or 70 episodes in order to have a larger amount of text to analyze for each show. Given these conditions, I decided to select the following TV shows: *South Park*, *The Simpsons*, *Futurama*, *American Dad!*, and *Family Guy*. Other shows that met the 70-episode threshold but were not selected include *Archer* and *King of the Hill*. The production studios for these shows had filed DMCA takedowns of publicly available transcripts, so they are not included. Other shows such as *Bob's Burgers* and *Aqua Teen Hunger Force* did not have complete transcripts of every episode online. \n",
    "\n",
    "There are five datasets, one for each TV show listed above, and each row of the dataset represents one line spoken in an episode of a given TV show. Therefore, a row includes the line spoken, the character speaking the line, the episode in which the line is spoken, and the season in which the episode occurs. \n",
    "\n",
    "While I have tried to fill in missing episode transcripts in order to have every episode represented in the datasets, there are some exceptions such as the *South Park* episodes \"200\" and \"201\" which has been banned for its depiction of the Islamic prophet Muhammad. Overall, these gaps in the dataset are relatively limited and do not significantly affect the accuracy of the conclusions presented. \n",
    "\n",
    "Additionally, all of these datasets can be found and downloaded on my [GitHub repo](https://github.com/bjablonski20/final-project-qtm340) along with all code used for the analysis in order to enable replication of the results below.\n",
    "\n",
    "The *Futurama*, *South Park*, and *The Simpsons* datasets were created by fans of the shows on Kaggle and GitHub, but *Family Guy* and *American Dad* were manually retrieved via web scraping using Beautiful Soup. \n",
    "\n",
    "The following code loads all data needed for this project from GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input",
     "hide-input",
     "hide-cell",
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## importing\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pandas import DataFrame\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.stats import pearsonr, norm\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "## South Park \n",
    "### retrieve csv from my github repo\n",
    "if not os.path.exists('sp_ep_data.csv.1'):\n",
    "    !wget https://raw.githubusercontent.com/bjablonski20/final-project-qtm340/main/Transcript%20CSVs/sp_ep_data.csv\n",
    "### convert into a pandas dataframe\n",
    "sp_ep_data = pd.read_csv ('sp_ep_data.csv.1',error_bad_lines=False)\n",
    "sp_ep_data = sp_ep_data.drop(['episode_link', 'season_link', 'season_name'], axis=1)\n",
    "### split dataframe into text files for each episode\n",
    "for season in range(1,sp_ep_data['season_number'].max()+1): ## season numbers\n",
    "    for episode in range(1, sp_ep_data[sp_ep_data['season_number'] == season]['season_episode_number'].max()):\n",
    "        filename = \"sp_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "        path = \"SouthPark_Data/\" + filename\n",
    "        with open(path, \"w\") as file:\n",
    "            file.writelines(sp_ep_data[(sp_ep_data['season_number'] == season) & (sp_ep_data['season_episode_number'] == episode)]['text'])\n",
    "       \n",
    "\n",
    " ##The Simpsons\n",
    "\n",
    "### retrieve csv from my github repo\n",
    "if not os.path.exists('simpsons_script_lines.csv'):\n",
    "    !wget https://raw.githubusercontent.com/bjablonski20/final-project-qtm340/main/Transcript%20CSVs/simpsons_script_lines.csv\n",
    "### convert into a pandas dataframe\n",
    "simp_ep_data = pd.read_csv ('simpsons_script_lines.csv',error_bad_lines=False)\n",
    "### Adding season column to simpsons dataset -- i know this is unwieldy but i decided to brute force it \n",
    "simp_ep_data['season'] = np.where(simp_ep_data['episode_id'] < (14),1,0)\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22))&(simp_ep_data['episode_id'] >= (14)),2,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24))&((simp_ep_data['episode_id'] >= (14+22))),3,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22))&(simp_ep_data['episode_id'] >= (14+22+24)),4,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22))&(simp_ep_data['episode_id'] >= (14+22+24+22)),5,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22)),6,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25)),7,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25)),8,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25)),9,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25)),10,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23)),11,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22)),12,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21)),13,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22)),14,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22)),15,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22)),16,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21)),17,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21+22)),18,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20))&(simp_ep_data['episode_id'] >= (401)),19,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21))&(simp_ep_data['episode_id'] >= (401+20)),20,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23))&(simp_ep_data['episode_id'] >= (401+20+21)),21,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22))&(simp_ep_data['episode_id'] >= (401+20+21+23)),22,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22+22))&(simp_ep_data['episode_id'] >= (401+20+21+23+22)),23,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22+22+22))&(simp_ep_data['episode_id'] >= (401+20+21+23+22+22)),24,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22+22+22+22))&(simp_ep_data['episode_id'] >= (401+20+21+23+22+22+22)),25,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] >= (401+20+21+23+22+22+22+22)),26,simp_ep_data['season'])\n",
    "### Split dataframe into individual text files for each episode\n",
    "for season in range(1,simp_ep_data['season'].max()+1): ## season numbers\n",
    "    if type(simp_ep_data[simp_ep_data['season'] == season]['episode_id'].max()) == type(simp_ep_data[simp_ep_data['season'] == 1]['episode_id'].max()):\n",
    "        for episode in range(int(simp_ep_data[simp_ep_data['season'] == season]['episode_id'].min()), int(simp_ep_data[simp_ep_data['season'] == season]['episode_id'].max())):\n",
    "            filename = \"simp_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "            path = \"Simpsons_Data/\" + filename\n",
    "            with open(path, \"w\") as file:\n",
    "                file.writelines(simp_ep_data[(simp_ep_data['season'] == season) & (simp_ep_data['episode_id'] == episode)]['spoken_words'])\n",
    "### these files were empty \n",
    "os.remove(\"./Simpsons_Data/simp_ep21_447_.txt\")\n",
    "os.remove(\"./Simpsons_Data/simp_ep20_424_.txt\")\n",
    "os.remove(\"./Simpsons_Data/simp_ep25_550_.txt\")\n",
    "                                  \n",
    "    \n",
    "\n",
    "\n",
    "## Futurama \n",
    "### retrieve csv from my github repo\n",
    "if not os.path.exists('futurama_ep_data.csv'):\n",
    "    !wget https://raw.githubusercontent.com/bjablonski20/final-project-qtm340/main/Transcript%20CSVs/futurama_ep_data.csv\n",
    "futur_ep_data = pd.read_csv ('futurama_ep_data.csv',error_bad_lines=False)\n",
    "\n",
    "### adds episode column\n",
    "episode = []\n",
    "ep_no = 1\n",
    "count = 0\n",
    "for index, row in futur_ep_data.iterrows(): \n",
    "    if count == 23811:\n",
    "        break\n",
    "    else:\n",
    "        if futur_ep_data['Episode'][count] != futur_ep_data['Episode'][count+1]:\n",
    "            episode.append(ep_no)\n",
    "            ep_no = ep_no +1\n",
    "            count = count +1\n",
    "        else: \n",
    "            count = count + 1\n",
    "            episode.append(ep_no)\n",
    "episode.append(114)\n",
    "futur_ep_data['Episode Number'] = episode\n",
    "\n",
    "### Splits the dataframe into individual text files for each episode \n",
    "for season in range(1,futur_ep_data['Season'].max()+1): ## season numbers\n",
    "    for episode in range(1, futur_ep_data[futur_ep_data['Season'] == season]['Episode Number'].max()):\n",
    "        filename = \"futur_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "        path = \"Futurama_Data/\" + filename\n",
    "        with open(path, \"w\") as file:\n",
    "            try:\n",
    "                file.writelines(futur_ep_data[(futur_ep_data['Season'] == season) & (futur_ep_data['Episode Number'] == episode)]['Line'])\n",
    "            except TypeError:\n",
    "                break\n",
    "### removes all empty episodes \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"Futurama_Data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(mypath + file) == 0:\n",
    "        os.remove(mypath + file)     \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Scraping \n",
    "\n",
    "### beautiful soup function that creates a pandas Dataframe with the episode, speaker and the line for the series \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re \n",
    "start = \"<b>\"\n",
    "end = \"</b>\"\n",
    "def grab_urls(soup):\n",
    "  episode_urls = []\n",
    "  for season in soup.findAll('div', style=\"column-count:2\"):\n",
    "    for episode in season.findAll('a'):\n",
    "        try:\n",
    "           episode_urls.append(episode.attrs['href'])\n",
    "        except:\n",
    "           continue\n",
    "  return episode_urls\n",
    "def scripts_from_html(html):\n",
    "    html_page = requests.get(html, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    urls = grab_urls(soup)\n",
    "    texts = []\n",
    "    characters = []\n",
    "    episode = []\n",
    "    ep_no = 1\n",
    "    character = \"NONE\"\n",
    "    for i in urls:\n",
    "        html = \"https://transcripts.fandom.com/\" + i\n",
    "        count = 0\n",
    "        html_page = requests.get(html)\n",
    "        html_string = html_page.text\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "        for i in soup.find_all(\"p\"):\n",
    "            text1 = str(i)\n",
    "            if \"<p>\" in text1:\n",
    "                text1 = text1[text1.index(\"<p>\")+len(\"<p>\"):text1.index(\"</p>\")]\n",
    "            episode.append(ep_no)\n",
    "            if \"</b>\" in str(i):\n",
    "                text2 = str(i)\n",
    "                character = text2[text2.index(\"<b>\")+len(\"<b>\"):text2.index(\"</b>\")]\n",
    "            characters.append(character)\n",
    "            count = count + 1\n",
    "            if \"</b>\" in text1:\n",
    "                text1 = text1[text1.index(\"</b>\")+len(\"</b>\"):len(text1)]\n",
    "            texts.append(text1)\n",
    "        ep_no = ep_no + 1\n",
    "\n",
    "    d = {'Character': characters,'Episode': episode, 'Line': texts}\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## American Dad\n",
    "html_amDad = \"https://transcripts.fandom.com/wiki/American_Dad!\"\n",
    "amDad_ep_data = scripts_from_html(html_amDad)\n",
    "## adds the season variable -- done manually because i couldnt find season number as a header when parsing through the html \n",
    "amDad_ep_data['season'] = np.where(amDad_ep_data['Episode'] < (23),1,0)\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19))&(amDad_ep_data['Episode'] >= (23)),2,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16))&(amDad_ep_data['Episode'] >= (23+19)),3,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20))&(amDad_ep_data['Episode'] >= (23+19+16)),4,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20+18))&(amDad_ep_data['Episode'] >= (23+19+16+20)),5,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20+18+19))&(amDad_ep_data['Episode'] >= (23+19+16+20+18)),6,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20+18+19+18))&(amDad_ep_data['Episode'] >= (23+19+16+20+18+19)),7,amDad_ep_data['season'])\n",
    "\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19))&(amDad_ep_data['Episode'] >= (133)),8,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20))&(amDad_ep_data['Episode'] >= (133+19)),9,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18))&(amDad_ep_data['Episode'] >= (133+19+20)),10,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22))&(amDad_ep_data['Episode'] >= (133+19+20+18)),11,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22+22))&(amDad_ep_data['Episode'] >= (133+19+20+18+22)),12,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22+22+22))&(amDad_ep_data['Episode'] >= (133+19+20+18+22+22+22)),13,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22+22+22+22))&(amDad_ep_data['Episode'] >= (133+19+20+18+22+22+22+22)),14,amDad_ep_data['season'])\n",
    "\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (278+22))&(amDad_ep_data['Episode'] >= (278)),15,amDad_ep_data['season'])\n",
    "\n",
    "## create individual .txt docs for each episode\n",
    "for season in range(1,amDad_ep_data['season'].max()+1): ## season numbers\n",
    "    try:\n",
    "        for episode in range(1, (int)(amDad_ep_data[amDad_ep_data['season'] == season]['Episode'].max())):\n",
    "            filename = \"amDad_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "            path = \"amDad_Data/\" + filename\n",
    "            with open(path, \"w\") as file:\n",
    "                file.writelines(amDad_ep_data[(amDad_ep_data['season'] == season) & (amDad_ep_data['Episode'] == episode)]['Line'])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "\n",
    "## removes all empty episodes \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"amDad_Data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(mypath + file) == 0:\n",
    "        os.remove(mypath + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Family Guy\n",
    "html_famGuy = \"https://transcripts.fandom.com/wiki/Family_Guy\"\n",
    "famGuy_ep_data = scripts_from_html(html_famGuy)\n",
    "famGuy_ep_data['season'] = np.where(famGuy_ep_data['Episode'] < (8),1,0)\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21))&(famGuy_ep_data['Episode'] >= (8)),2,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22))&(famGuy_ep_data['Episode'] >= (8+21)),3,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30))&(famGuy_ep_data['Episode'] >= (8+21+22)),4,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30+18))&(famGuy_ep_data['Episode'] >= (8+21+22+30)),5,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30+18+12))&(famGuy_ep_data['Episode'] >= (8+21+22+30+18)),6,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30+18+12+16))&(famGuy_ep_data['Episode'] >= (8+21+22+30+18+12)),7,famGuy_ep_data['season'])\n",
    "\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21))&(famGuy_ep_data['Episode'] >= (112)),8,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18))&(famGuy_ep_data['Episode'] >= (112+21)),9,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23))&(famGuy_ep_data['Episode'] >= (112+21+18)),10,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22))&(famGuy_ep_data['Episode'] >= (112+21+18+23)),11,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22+21))&(famGuy_ep_data['Episode'] >= (112+21+18+23+22)),12,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22+21+18))&(famGuy_ep_data['Episode'] >= (112+21+18+23+22+21)),13,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22+21+18+20))&(famGuy_ep_data['Episode'] >= (112+21+18+23+22+21+18)),14,famGuy_ep_data['season'])\n",
    "\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20))&(famGuy_ep_data['Episode'] >= (255)),15,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20))&(famGuy_ep_data['Episode'] >= (255+20)),16,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20+20))&(famGuy_ep_data['Episode'] >= (255+20+20)),17,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20+20+20))&(famGuy_ep_data['Episode'] >= (255+20+20+20)),18,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20+20+20+22))&(famGuy_ep_data['Episode'] >= (255+20+20+20+20)),19,famGuy_ep_data['season'])\n",
    "\n",
    "\n",
    "## creates individual .txt files for each episode\n",
    "for season in range(1,famGuy_ep_data['season'].max()+1): ## season numbers\n",
    "    try:\n",
    "        for episode in range(1, (int)(famGuy_ep_data[famGuy_ep_data['season'] == season]['Episode'].max())):\n",
    "            filename = \"famGuy_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "            path = \"famGuy_Data/\" + filename\n",
    "            with open(path, \"w\") as file:\n",
    "                file.writelines(famGuy_ep_data[(famGuy_ep_data['season'] == season) & (famGuy_ep_data['Episode'] == episode)]['Line'])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "\n",
    "## removes empty files \n",
    "mypath = \"famGuy_Data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(mypath + file) == 0:\n",
    "        os.remove(mypath + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Method\n",
    "I created three different classification models. The models differentiate between *American Dad!* and *Family Guy* episodes, *Simpsons* and *Futurama* episodes, and *South Park* and *Simpsons* episodes. The first two pairs are selected because the shows in each pair are created and written by many of the same people while the last pair is picked because *South Park* and *The Simpsons* are the two longest running shows in the dataset. Additionally, stop words are taken out of the text, including show specific place and character names. The stop words used for this analysis can also be downloaded on my GitHub repo. \n",
    "\n",
    "The goal of this section is both to test whether the classification method can differentiate between similar TV shows based only on the text of an episode of the show and also to find which words are most important in differentiating between any two TV shows. The relative ability of the classification model to distinguish between episodes of specific shows can provide an indicator of the similarity of two shows which is important to identify the diversity of humor and theme between two shows. Additionally, identifying which episodes the model repeatedly misidentifies provides opportunity for close reading by looking at the text of the show and identifying *why* the classification model has failed. The logistic regression weights given to individual words indicates the importance of words to the identification of a show. While it is an imperfect measure, the highly weighted words can potentially paint an overall theme that differentiates a show from its counterpart in the model. \n",
    "\n",
    "\n",
    "#### *Family Guy*-*American Dad!* Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Family Guy-American Dad\n",
    "### General Things \n",
    "### load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('./docs/stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "### create dtm\n",
    "corpus_path = './Classification_Data_AmDadFamGuy/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')\n",
    "\n",
    "\n",
    "directory = \"./Classification_Data_AmDadFamGuy/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "titles = [Path(file).stem for file in files]\n",
    "\n",
    "corpus = []\n",
    "for title in titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "fulltext = []\n",
    "for filepath in glob.iglob('Classification_Data_AmDadFamGuy/*.txt'): ## grab titles\n",
    "    filepaths.append(filepath)\n",
    "for filepath in filepaths: ## grab text and get the scores of each individual episode\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        fulltext.append(text)\n",
    "clasdict = {'Name': filepaths, 'Text': fulltext\n",
    "}\n",
    "classdf = pd.DataFrame(clasdict)\n",
    "def changeFilePath(filePath):\n",
    "    newPath = filePath[32:]\n",
    "    return newPath\n",
    "classdf['Name'] = classdf['Name'].apply(changeFilePath)\n",
    "amDadBool = []\n",
    "\n",
    "for name in classdf['Name']:\n",
    "    if name[0:4] == \"amDa\":\n",
    "        amDadBool.append(1)\n",
    "    else:\n",
    "        amDadBool.append(0)\n",
    "classdf['amDad'] = amDadBool\n",
    "\n",
    "df_concat = pd.concat([classdf, df], axis = 1)\n",
    "noAD = df_concat['amDad'].sum()\n",
    "noAD = int(noAD)\n",
    "df_simp = df_concat[df_concat['amDad'] == 0]\n",
    "df_sp = df_concat[df_concat['amDad'] == 1]\n",
    "df_simp = df_simp.sample(n=noAD)\n",
    "df_final = pd.concat([df_simp, df_sp])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "meta = df_final[[\"Name\", \"amDad\"]]\n",
    "df = df_final.loc[:,'000':]\n",
    "\n",
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''\n",
    "\n",
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')\n",
    "\n",
    "\n",
    "for this_index in df_final.index.tolist():\n",
    "    title = meta.loc[meta.index[this_index], 'Name'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'amDad']\n",
    "    #print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'amDad'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "sum_column = meta['amDad'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "\n",
    "sum_column = meta['amDad'] - meta['PREDICTED']\n",
    "meta['Result'] = sum_column\n",
    "\n",
    "\n",
    "meta[meta['RESULT'] == 1]\n",
    "\n",
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['amDad']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['amDad']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['amDad']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "feat_df = feat_pval_weight(meta, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Family Guy-American Dad model is 0.9162790697674419\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(meta[meta['RESULT'] == 0])/len(meta)\n",
    "print(\"The accuracy of the Family Guy-American Dad model is \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>usa</td>\n",
       "      <td>2.588796e-30</td>\n",
       "      <td>-1.141640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>cia</td>\n",
       "      <td>4.619195e-11</td>\n",
       "      <td>-0.579678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>alien</td>\n",
       "      <td>2.602196e-04</td>\n",
       "      <td>-0.431902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>need</td>\n",
       "      <td>1.935979e-04</td>\n",
       "      <td>-0.430273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>morning</td>\n",
       "      <td>2.002740e-15</td>\n",
       "      <td>-0.406824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>american</td>\n",
       "      <td>1.272633e-10</td>\n",
       "      <td>-0.370146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>crazy</td>\n",
       "      <td>7.749670e-03</td>\n",
       "      <td>-0.357220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3928</th>\n",
       "      <td>worry</td>\n",
       "      <td>1.449264e-01</td>\n",
       "      <td>-0.353994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>didn</td>\n",
       "      <td>5.865955e-03</td>\n",
       "      <td>-0.257974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>aah</td>\n",
       "      <td>2.638893e-02</td>\n",
       "      <td>-0.229395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>loser</td>\n",
       "      <td>4.228593e-02</td>\n",
       "      <td>-0.223178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3911</th>\n",
       "      <td>won</td>\n",
       "      <td>5.951847e-02</td>\n",
       "      <td>-0.204051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>took</td>\n",
       "      <td>4.334746e-01</td>\n",
       "      <td>-0.192996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>fish</td>\n",
       "      <td>1.150825e-01</td>\n",
       "      <td>-0.173260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3679</th>\n",
       "      <td>time</td>\n",
       "      <td>4.191625e-01</td>\n",
       "      <td>-0.166685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>play</td>\n",
       "      <td>7.128104e-01</td>\n",
       "      <td>-0.161909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2426</th>\n",
       "      <td>isn</td>\n",
       "      <td>6.725430e-02</td>\n",
       "      <td>-0.159112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>trying</td>\n",
       "      <td>7.061476e-01</td>\n",
       "      <td>-0.147032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>true</td>\n",
       "      <td>2.207401e-01</td>\n",
       "      <td>-0.145334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>ho</td>\n",
       "      <td>4.432051e-01</td>\n",
       "      <td>-0.143775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT       P_VALUE  LR_WEIGHT\n",
       "3779       usa  2.588796e-30  -1.141640\n",
       "1549       cia  4.619195e-11  -0.579678\n",
       "1143     alien  2.602196e-04  -0.431902\n",
       "2770      need  1.935979e-04  -0.430273\n",
       "2724   morning  2.002740e-15  -0.406824\n",
       "1150  american  1.272633e-10  -0.370146\n",
       "1682     crazy  7.749670e-03  -0.357220\n",
       "3928     worry  1.449264e-01  -0.353994\n",
       "1772      didn  5.865955e-03  -0.257974\n",
       "1087       aah  2.638893e-02  -0.229395\n",
       "2604     loser  4.228593e-02  -0.223178\n",
       "3911       won  5.951847e-02  -0.204051\n",
       "3694      took  4.334746e-01  -0.192996\n",
       "2052      fish  1.150825e-01  -0.173260\n",
       "3679      time  4.191625e-01  -0.166685\n",
       "2960      play  7.128104e-01  -0.161909\n",
       "2426       isn  6.725430e-02  -0.159112\n",
       "3739    trying  7.061476e-01  -0.147032\n",
       "3734      true  2.207401e-01  -0.145334\n",
       "2323        ho  4.432051e-01  -0.143775"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3878</th>\n",
       "      <td>white</td>\n",
       "      <td>2.833020e-01</td>\n",
       "      <td>0.130519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>grunting</td>\n",
       "      <td>5.742590e-01</td>\n",
       "      <td>0.145611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2517</th>\n",
       "      <td>laugh</td>\n",
       "      <td>3.555786e-24</td>\n",
       "      <td>0.152365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>mean</td>\n",
       "      <td>1.038660e-07</td>\n",
       "      <td>0.156990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>guys</td>\n",
       "      <td>4.260305e-16</td>\n",
       "      <td>0.172757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>happy</td>\n",
       "      <td>1.843361e-01</td>\n",
       "      <td>0.188815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>huh</td>\n",
       "      <td>6.124952e-09</td>\n",
       "      <td>0.194199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>trouble</td>\n",
       "      <td>4.781575e-03</td>\n",
       "      <td>0.209467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>mayor</td>\n",
       "      <td>4.148590e-03</td>\n",
       "      <td>0.214255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>violence</td>\n",
       "      <td>3.336212e-82</td>\n",
       "      <td>0.222522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>movies</td>\n",
       "      <td>9.443529e-37</td>\n",
       "      <td>0.223785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>moves</td>\n",
       "      <td>3.565026e-01</td>\n",
       "      <td>0.238898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>right</td>\n",
       "      <td>5.673501e-25</td>\n",
       "      <td>0.291620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>video</td>\n",
       "      <td>1.222821e-02</td>\n",
       "      <td>0.347011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>fat</td>\n",
       "      <td>1.526307e-05</td>\n",
       "      <td>0.365241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>hell</td>\n",
       "      <td>1.463040e-19</td>\n",
       "      <td>0.403359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>theme</td>\n",
       "      <td>3.941656e-04</td>\n",
       "      <td>0.421022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>today</td>\n",
       "      <td>4.740959e-17</td>\n",
       "      <td>0.508683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>tv</td>\n",
       "      <td>3.631870e-19</td>\n",
       "      <td>0.665700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>lucky</td>\n",
       "      <td>1.924432e-53</td>\n",
       "      <td>1.169684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT       P_VALUE  LR_WEIGHT\n",
       "3878     white  2.833020e-01   0.130519\n",
       "2223  grunting  5.742590e-01   0.145611\n",
       "2517     laugh  3.555786e-24   0.152365\n",
       "2662      mean  1.038660e-07   0.156990\n",
       "2239      guys  4.260305e-16   0.172757\n",
       "2262     happy  1.843361e-01   0.188815\n",
       "2363       huh  6.124952e-09   0.194199\n",
       "3732   trouble  4.781575e-03   0.209467\n",
       "2659     mayor  4.148590e-03   0.214255\n",
       "3797  violence  3.336212e-82   0.222522\n",
       "2736    movies  9.443529e-37   0.223785\n",
       "2734     moves  3.565026e-01   0.238898\n",
       "3170     right  5.673501e-25   0.291620\n",
       "3794     video  1.222821e-02   0.347011\n",
       "2003       fat  1.526307e-05   0.365241\n",
       "2295      hell  1.463040e-19   0.403359\n",
       "3654     theme  3.941656e-04   0.421022\n",
       "3685     today  4.740959e-17   0.508683\n",
       "3750        tv  3.631870e-19   0.665700\n",
       "2624     lucky  1.924432e-53   1.169684"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Futurama*-*The Simpsons* Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Futurama-Simpsons\n",
    "## General Things \n",
    "# load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('./docs/stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "# create dtm\n",
    "corpus_path = './Classification_Data_SimpFuturama/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')\n",
    "\n",
    "\n",
    "directory = \"./Classification_Data_SimpFuturama/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "titles = [Path(file).stem for file in files]\n",
    "\n",
    "corpus = []\n",
    "for title in titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "fulltext = []\n",
    "for filepath in glob.iglob('Classification_Data_SimpFuturama/*.txt'): ## grab titles\n",
    "    filepaths.append(filepath)\n",
    "for filepath in filepaths: ## grab text and get the scores of each individual episode\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        fulltext.append(text)\n",
    "clasdict = {'Name': filepaths, 'Text': fulltext\n",
    "}\n",
    "classdf = pd.DataFrame(clasdict)\n",
    "def changeFilePath(filePath):\n",
    "    newPath = filePath[33:]\n",
    "    return newPath\n",
    "classdf['Name'] = classdf['Name'].apply(changeFilePath)\n",
    "futramaBool = []\n",
    "\n",
    "for name in classdf['Name']:\n",
    "    if name[0:4] == \"simp\":\n",
    "        futramaBool.append(0)\n",
    "    else:\n",
    "        futramaBool.append(1)\n",
    "classdf['Futurama'] = futramaBool\n",
    "\n",
    "df_concat = pd.concat([classdf, df], axis = 1)\n",
    "noF = df_concat['Futurama'].sum()\n",
    "\n",
    "df_simp = df_concat[df_concat['Futurama'] == 0]\n",
    "df_sp = df_concat[df_concat['Futurama'] == 1]\n",
    "df_simp = df_simp.sample(n=noF)\n",
    "df_final = pd.concat([df_simp, df_sp])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "meta = df_final[[\"Name\", \"Futurama\"]]\n",
    "df = df_final.loc[:,'000':]\n",
    "\n",
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''\n",
    "\n",
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')\n",
    "\n",
    "\n",
    "for this_index in df_final.index.tolist():\n",
    "    title = meta.loc[meta.index[this_index], 'Name'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'Futurama']\n",
    "    #print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'Futurama'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "sum_column = meta['Futurama'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "\n",
    "sum_column = meta['Futurama'] - meta['PREDICTED']\n",
    "meta['Result'] = sum_column\n",
    "\n",
    "\n",
    "meta[meta['RESULT'] == 1]\n",
    "\n",
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['Futurama']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['Futurama']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['Futurama']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "feat_df = feat_pval_weight(meta, df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Futurama-Simpsons model is: 0.9375\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(meta[meta['RESULT'] == 0])/len(meta)\n",
    "print(\"The accuracy of the Futurama-Simpsons model is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>planet</td>\n",
       "      <td>2.125784e-10</td>\n",
       "      <td>-0.974364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2370</th>\n",
       "      <td>robot</td>\n",
       "      <td>2.366492e-11</td>\n",
       "      <td>-0.795632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>professor</td>\n",
       "      <td>3.293145e-07</td>\n",
       "      <td>-0.774413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>space</td>\n",
       "      <td>2.360484e-05</td>\n",
       "      <td>-0.468828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>delivery</td>\n",
       "      <td>6.850940e-06</td>\n",
       "      <td>-0.331318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>things</td>\n",
       "      <td>2.060697e-03</td>\n",
       "      <td>-0.291092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>wanna</td>\n",
       "      <td>2.764642e-04</td>\n",
       "      <td>-0.261212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>ass</td>\n",
       "      <td>1.117970e-06</td>\n",
       "      <td>-0.249741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>surface</td>\n",
       "      <td>2.312275e-02</td>\n",
       "      <td>-0.217342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>ship</td>\n",
       "      <td>3.045339e-05</td>\n",
       "      <td>-0.212384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>universe</td>\n",
       "      <td>1.631520e-03</td>\n",
       "      <td>-0.181786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>coffee</td>\n",
       "      <td>4.598779e-01</td>\n",
       "      <td>-0.176244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>captain</td>\n",
       "      <td>2.506399e-03</td>\n",
       "      <td>-0.163081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>alien</td>\n",
       "      <td>1.251765e-01</td>\n",
       "      <td>-0.147831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>wait</td>\n",
       "      <td>1.960257e-03</td>\n",
       "      <td>-0.135687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>eye</td>\n",
       "      <td>1.483396e-01</td>\n",
       "      <td>-0.124693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>news</td>\n",
       "      <td>2.096116e-04</td>\n",
       "      <td>-0.103324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>human</td>\n",
       "      <td>1.216860e-05</td>\n",
       "      <td>-0.070077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>warning</td>\n",
       "      <td>7.494784e-01</td>\n",
       "      <td>-0.046182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>god</td>\n",
       "      <td>3.056399e-01</td>\n",
       "      <td>-0.038779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FEAT       P_VALUE  LR_WEIGHT\n",
       "2114     planet  2.125784e-10  -0.974364\n",
       "2370      robot  2.366492e-11  -0.795632\n",
       "2214  professor  3.293145e-07  -0.774413\n",
       "2655      space  2.360484e-05  -0.468828\n",
       "757    delivery  6.850940e-06  -0.331318\n",
       "2881     things  2.060697e-03  -0.291092\n",
       "3067      wanna  2.764642e-04  -0.261212\n",
       "127         ass  1.117970e-06  -0.249741\n",
       "2798    surface  2.312275e-02  -0.217342\n",
       "2520       ship  3.045339e-05  -0.212384\n",
       "3014   universe  1.631520e-03  -0.181786\n",
       "559      coffee  4.598779e-01  -0.176244\n",
       "413     captain  2.506399e-03  -0.163081\n",
       "65        alien  1.251765e-01  -0.147831\n",
       "3054       wait  1.960257e-03  -0.135687\n",
       "1012        eye  1.483396e-01  -0.124693\n",
       "1920       news  2.096116e-04  -0.103324\n",
       "1448      human  1.216860e-05  -0.070077\n",
       "3076    warning  7.494784e-01  -0.046182\n",
       "1241        god  3.056399e-01  -0.038779"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>festival</td>\n",
       "      <td>1.718463e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>mom</td>\n",
       "      <td>1.907975e-03</td>\n",
       "      <td>0.002129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>beer</td>\n",
       "      <td>7.911002e-02</td>\n",
       "      <td>0.005314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>dream</td>\n",
       "      <td>2.870535e-01</td>\n",
       "      <td>0.012255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>little</td>\n",
       "      <td>4.464262e-05</td>\n",
       "      <td>0.016235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3172</th>\n",
       "      <td>work</td>\n",
       "      <td>1.939321e-01</td>\n",
       "      <td>0.026852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>school</td>\n",
       "      <td>2.758281e-07</td>\n",
       "      <td>0.056057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>book</td>\n",
       "      <td>5.541133e-05</td>\n",
       "      <td>0.062601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>kids</td>\n",
       "      <td>3.093289e-05</td>\n",
       "      <td>0.086007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>family</td>\n",
       "      <td>1.264796e-07</td>\n",
       "      <td>0.088111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>fair</td>\n",
       "      <td>1.412211e-02</td>\n",
       "      <td>0.102877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>heh</td>\n",
       "      <td>3.893409e-03</td>\n",
       "      <td>0.102976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>house</td>\n",
       "      <td>1.430128e-04</td>\n",
       "      <td>0.111072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>right</td>\n",
       "      <td>2.752455e-09</td>\n",
       "      <td>0.143036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3179</th>\n",
       "      <td>world</td>\n",
       "      <td>2.741750e-01</td>\n",
       "      <td>0.161714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>boy</td>\n",
       "      <td>6.529077e-08</td>\n",
       "      <td>0.211274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>job</td>\n",
       "      <td>5.387742e-01</td>\n",
       "      <td>0.250658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3068</th>\n",
       "      <td>want</td>\n",
       "      <td>1.252064e-06</td>\n",
       "      <td>0.254407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>night</td>\n",
       "      <td>3.563865e-02</td>\n",
       "      <td>0.304644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>dad</td>\n",
       "      <td>1.711793e-18</td>\n",
       "      <td>0.765774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT       P_VALUE  LR_WEIGHT\n",
       "1076  festival  1.718463e-02   0.000000\n",
       "1846       mom  1.907975e-03   0.002129\n",
       "223       beer  7.911002e-02   0.005314\n",
       "852      dream  2.870535e-01   0.012255\n",
       "1681    little  4.464262e-05   0.016235\n",
       "3172      work  1.939321e-01   0.026852\n",
       "2441    school  2.758281e-07   0.056057\n",
       "293       book  5.541133e-05   0.062601\n",
       "1571      kids  3.093289e-05   0.086007\n",
       "1034    family  1.264796e-07   0.088111\n",
       "1024      fair  1.412211e-02   0.102877\n",
       "1365       heh  3.893409e-03   0.102976\n",
       "1442     house  1.430128e-04   0.111072\n",
       "2359     right  2.752455e-09   0.143036\n",
       "3179     world  2.741750e-01   0.161714\n",
       "318        boy  6.529077e-08   0.211274\n",
       "1536       job  5.387742e-01   0.250658\n",
       "3068      want  1.252064e-06   0.254407\n",
       "1926     night  3.563865e-02   0.304644\n",
       "713        dad  1.711793e-18   0.765774"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *South Park*-*The Simpsons* Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## South Park-Simpsons\n",
    "## General Things \n",
    "# load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('./docs/stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "# create dtm\n",
    "corpus_path = './Classification_Data/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')\n",
    "\n",
    "\n",
    "directory = \"./Classification_Data/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "titles = [Path(file).stem for file in files]\n",
    "\n",
    "corpus = []\n",
    "for title in titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "fulltext = []\n",
    "for filepath in glob.iglob('Classification_Data/*.txt'): ## grab titles\n",
    "    filepaths.append(filepath)\n",
    "for filepath in filepaths: ## grab text and get the scores of each individual episode\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        fulltext.append(text)\n",
    "clasdict = {'Name': filepaths, 'Text': fulltext\n",
    "}\n",
    "classdf = pd.DataFrame(clasdict)\n",
    "def changeFilePath(filePath):\n",
    "    newPath = filePath[20:]\n",
    "    return newPath\n",
    "classdf['Name'] = classdf['Name'].apply(changeFilePath)\n",
    "southParkBool = []\n",
    "\n",
    "for name in classdf['Name']:\n",
    "    if name[0:4] == \"simp\":\n",
    "        southParkBool.append(0)\n",
    "    else:\n",
    "        southParkBool.append(1)\n",
    "classdf['SouthPark'] = southParkBool\n",
    "\n",
    "df_concat = pd.concat([classdf, df], axis = 1)\n",
    "noSP = df_concat['SouthPark'].sum()\n",
    "\n",
    "df_simp = df_concat[df_concat['SouthPark'] == 0]\n",
    "df_sp = df_concat[df_concat['SouthPark'] == 1]\n",
    "df_simp = df_simp.sample(n=noSP)\n",
    "df_final = pd.concat([df_simp, df_sp])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "meta = df_final[[\"Name\", \"SouthPark\"]]\n",
    "df = df_final.loc[:,'000':]\n",
    "\n",
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''\n",
    "\n",
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')\n",
    "\n",
    "\n",
    "for this_index in df_final.index.tolist():\n",
    "    title = meta.loc[meta.index[this_index], 'Name'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'SouthPark']\n",
    "    #print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'SouthPark'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "sum_column = meta['SouthPark'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "\n",
    "sum_column = meta['SouthPark'] - meta['PREDICTED']\n",
    "meta['Result'] = sum_column\n",
    "\n",
    "\n",
    "meta[meta['RESULT'] == 1]\n",
    "\n",
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['SouthPark']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['SouthPark']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['SouthPark']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "feat_df = feat_pval_weight(meta, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of The South Park-Simpsons Model is: 0.9868421052631579\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(meta[meta['RESULT'] == 0])/len(meta)\n",
    "print(\"The accuracy of The South Park-Simpsons Model is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>walks</td>\n",
       "      <td>1.023394e-162</td>\n",
       "      <td>-0.655405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>turns</td>\n",
       "      <td>7.402690e-140</td>\n",
       "      <td>-0.569603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>dude</td>\n",
       "      <td>7.295699e-111</td>\n",
       "      <td>-0.482903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>takes</td>\n",
       "      <td>8.699849e-89</td>\n",
       "      <td>-0.388868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>away</td>\n",
       "      <td>7.299929e-116</td>\n",
       "      <td>-0.283372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>monitor</td>\n",
       "      <td>3.230522e-04</td>\n",
       "      <td>-0.255395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>ass</td>\n",
       "      <td>1.234175e-29</td>\n",
       "      <td>-0.241539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>looks</td>\n",
       "      <td>4.888705e-116</td>\n",
       "      <td>-0.159034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>canada</td>\n",
       "      <td>2.300872e-02</td>\n",
       "      <td>-0.141829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>guys</td>\n",
       "      <td>7.788472e-67</td>\n",
       "      <td>-0.137286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>children</td>\n",
       "      <td>5.368362e-10</td>\n",
       "      <td>-0.120084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>gun</td>\n",
       "      <td>1.891196e-05</td>\n",
       "      <td>-0.110453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>chef</td>\n",
       "      <td>1.169461e-04</td>\n",
       "      <td>-0.098878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>boys</td>\n",
       "      <td>1.813512e-35</td>\n",
       "      <td>-0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>begins</td>\n",
       "      <td>4.767993e-73</td>\n",
       "      <td>-0.086177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>gets</td>\n",
       "      <td>5.029916e-73</td>\n",
       "      <td>-0.084186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2758</th>\n",
       "      <td>pig</td>\n",
       "      <td>1.307452e-01</td>\n",
       "      <td>-0.071826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>door</td>\n",
       "      <td>2.436828e-78</td>\n",
       "      <td>-0.071806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>people</td>\n",
       "      <td>2.999160e-43</td>\n",
       "      <td>-0.068959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>park</td>\n",
       "      <td>1.968263e-17</td>\n",
       "      <td>-0.068741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT        P_VALUE  LR_WEIGHT\n",
       "4120     walks  1.023394e-162  -0.655405\n",
       "4007     turns  7.402690e-140  -0.569603\n",
       "1200      dude  7.295699e-111  -0.482903\n",
       "3783     takes   8.699849e-89  -0.388868\n",
       "231       away  7.299929e-116  -0.283372\n",
       "2439   monitor   3.230522e-04  -0.255395\n",
       "194        ass   1.234175e-29  -0.241539\n",
       "2265     looks  4.888705e-116  -0.159034\n",
       "563     canada   2.300872e-02  -0.141829\n",
       "1752      guys   7.788472e-67  -0.137286\n",
       "687   children   5.368362e-10  -0.120084\n",
       "1748       gun   1.891196e-05  -0.110453\n",
       "676       chef   1.169461e-04  -0.098878\n",
       "447       boys   1.813512e-35  -0.095300\n",
       "331     begins   4.767993e-73  -0.086177\n",
       "1629      gets   5.029916e-73  -0.084186\n",
       "2758       pig   1.307452e-01  -0.071826\n",
       "1144      door   2.436828e-78  -0.071806\n",
       "2720    people   2.999160e-43  -0.068959\n",
       "2683      park   1.968263e-17  -0.068741"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>hot</td>\n",
       "      <td>3.868896e-01</td>\n",
       "      <td>0.075627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>fine</td>\n",
       "      <td>6.462251e-03</td>\n",
       "      <td>0.079655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>super</td>\n",
       "      <td>1.314430e-02</td>\n",
       "      <td>0.080141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>look</td>\n",
       "      <td>5.866443e-45</td>\n",
       "      <td>0.082228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3851</th>\n",
       "      <td>thanks</td>\n",
       "      <td>6.286511e-02</td>\n",
       "      <td>0.092077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>chief</td>\n",
       "      <td>1.990508e-06</td>\n",
       "      <td>0.093275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>life</td>\n",
       "      <td>2.314188e-01</td>\n",
       "      <td>0.095925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>baby</td>\n",
       "      <td>3.167185e-02</td>\n",
       "      <td>0.114769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>gotta</td>\n",
       "      <td>2.027379e-05</td>\n",
       "      <td>0.129279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>boy</td>\n",
       "      <td>1.730123e-01</td>\n",
       "      <td>0.130369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>girl</td>\n",
       "      <td>3.120655e-01</td>\n",
       "      <td>0.133866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740</th>\n",
       "      <td>guess</td>\n",
       "      <td>2.572595e-02</td>\n",
       "      <td>0.134050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4125</th>\n",
       "      <td>want</td>\n",
       "      <td>3.712520e-07</td>\n",
       "      <td>0.137909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>wife</td>\n",
       "      <td>7.082704e-02</td>\n",
       "      <td>0.153016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>house</td>\n",
       "      <td>9.073806e-03</td>\n",
       "      <td>0.171647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3850</th>\n",
       "      <td>thank</td>\n",
       "      <td>3.141641e-01</td>\n",
       "      <td>0.184251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>years</td>\n",
       "      <td>2.089192e-01</td>\n",
       "      <td>0.199317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>kids</td>\n",
       "      <td>1.279490e-08</td>\n",
       "      <td>0.215028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>day</td>\n",
       "      <td>4.163605e-01</td>\n",
       "      <td>0.306515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>dad</td>\n",
       "      <td>1.206554e-08</td>\n",
       "      <td>0.324747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FEAT       P_VALUE  LR_WEIGHT\n",
       "1919     hot  3.868896e-01   0.075627\n",
       "1455    fine  6.462251e-03   0.079655\n",
       "3732   super  1.314430e-02   0.080141\n",
       "2261    look  5.866443e-45   0.082228\n",
       "3851  thanks  6.286511e-02   0.092077\n",
       "684    chief  1.990508e-06   0.093275\n",
       "2207    life  2.314188e-01   0.095925\n",
       "246     baby  3.167185e-02   0.114769\n",
       "1687   gotta  2.027379e-05   0.129279\n",
       "445      boy  1.730123e-01   0.130369\n",
       "1641    girl  3.120655e-01   0.133866\n",
       "1740   guess  2.572595e-02   0.134050\n",
       "4125    want  3.712520e-07   0.137909\n",
       "4211    wife  7.082704e-02   0.153016\n",
       "1924   house  9.073806e-03   0.171647\n",
       "3850   thank  3.141641e-01   0.184251\n",
       "4299   years  2.089192e-01   0.199317\n",
       "2094    kids  1.279490e-08   0.215028\n",
       "1001     day  4.163605e-01   0.306515\n",
       "975      dad  1.206554e-08   0.324747"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Method \n",
    "I created five separate Word2Vec models with each model being trained on every script of an individual TV show. This technique enables parsing out vocabulary and thematic differences between TV shows by investigating the similar words in one model compared to another, and it additionally provides a show's provisional definition of a word by displaying which words are similar in the vocabulary of that show. \n",
    "\n",
    "While the classification section attempts to define and differentiate shows on a binary basis (i.e. What themes and words are uniquely characteristic of *The Simpsons* rather than *Futurama*?), this section primarily seeks to find unique differences in the vocabulary of an individual TV show in relation to the four other shows in the dataset. This is, by nature, much more open ended, seeking to identify major themes that can uniquely characterize the writing of an individual TV show. \n",
    "\n",
    "The following code trains all five word2vec models and then displays the 10 most similar words to the word \"girl\", \"science\" and \"family\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "## South Park\n",
    "base_dir = \"./SouthPark_Data/\" \n",
    "\n",
    "all_docs = [] \n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Get titles\n",
    "directory = \"./SouthPark_Data/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "sp_titles = [Path(file).stem for file in files]\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "       # print(sp_titles[counter]) # let's print the title of the obit\n",
    "       # print(len(sentences))  # let's check how many sentences there are per obit\n",
    "        #print(\"\\n\")\n",
    "        counter += 1\n",
    "    return all_txt\n",
    "\n",
    "sentences = make_sentences(all_docs)\n",
    "\n",
    "# trains our model!\n",
    "sp_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, \n",
    "    size=200,\n",
    "    workers=5) # parallel processing; \n",
    "sp_model.save('sp_model') ## saves the model\n",
    "\n",
    "## Simpsons\n",
    "\n",
    "\n",
    "base_dir = \"./Simpsons_Data/\" \n",
    "\n",
    "all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# need our handy nltk tokenizer \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and we'll get titles\n",
    "directory = \"./Simpsons_Data/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "simp_titles = [Path(file).stem for file in files]\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "        #print(simp_titles[counter]) # let's print the title of the obit\n",
    "       # print(len(sentences))  # let's check how many sentences there are per obit\n",
    "       # print(\"\\n\")\n",
    "        counter += 1\n",
    "    return all_txt\n",
    "\n",
    "sentences = make_sentences(all_docs)\n",
    "\n",
    "# let's train our model!\n",
    "simp_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "simp_model.save('simp_model')\n",
    "\n",
    "\n",
    "\n",
    "## Family Guy\n",
    "base_dir = \"./famGuy_Data/\" \n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "    return all_txt\n",
    "def createW2VSentences(dire):\n",
    "    all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "    docs = os.listdir(dire) # get a list of all the files in the directory\n",
    "\n",
    "    for doc in docs: # iterate through the docs\n",
    "        if not doc.startswith('.'): # get only the .txt files\n",
    "            with open(dire + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "                text = file.read() # read in the file as a single text string\n",
    "                all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# need our handy nltk tokenizer \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and we'll get titles\n",
    "    files = glob.glob(f\"{dire}/*.txt\")\n",
    "    par_titles = [Path(file).stem for file in files]\n",
    "\n",
    "    sentences = make_sentences(all_docs)\n",
    "    return sentences\n",
    "\n",
    "# let's train our model!\n",
    "famGuyDir = \"./famGuy_Data/\"\n",
    "sentences = createW2VSentences(famGuyDir)\n",
    "\n",
    "famGuy_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "famGuy_model.save('famGuy_model')\n",
    "\n",
    "## American Dad\n",
    "amDadDir = \"./amDad_Data/\"\n",
    "sentences = createW2VSentences(amDadDir)\n",
    "\n",
    "amDad_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "amDad_model.save('amDad_model')\n",
    "\n",
    "## Futurama\n",
    "futurDir = \"./Futurama_Data/\"\n",
    "sentences = createW2VSentences(futurDir)\n",
    "\n",
    "futur_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "futur_model.save('futur_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for the word: girl\n",
      "\n",
      "Futurama model: \n",
      "\n",
      "[('murder', 0.9974381923675537), ('information', 0.9971227645874023), ('clear', 0.9971027374267578), ('lying', 0.9970874786376953), ('kinda', 0.9968289136886597), ('heat', 0.9967789649963379), ('xmas', 0.9966915249824524), ('worse', 0.9965395927429199), ('acting', 0.996530294418335), ('hungry', 0.9964852333068848)]\n",
      "\n",
      "\n",
      "American Dad model: \n",
      "\n",
      "[('kid', 0.866576075553894), ('dog', 0.8120333552360535), ('joke', 0.8057672381401062), ('alien', 0.7916181683540344), ('movie', 0.7905248403549194), ('little', 0.7783411741256714), ('lady', 0.7713030576705933), ('guy', 0.7710555791854858), ('mistake', 0.7640916109085083), ('person', 0.7620455026626587)]\n",
      "\n",
      "\n",
      "Family Guy model: \n",
      "\n",
      "[('kid', 0.8343309164047241), ('woman', 0.8112009763717651), ('lady', 0.8006547093391418), ('dog', 0.7698657512664795), ('chick', 0.7621921896934509), ('person', 0.7526925802230835), ('guy', 0.7453711032867432), ('joke', 0.7126767635345459), ('big', 0.6856566667556763), ('man', 0.6847807168960571)]\n",
      "\n",
      "\n",
      "Simpsons model: \n",
      "\n",
      "[('kid', 0.8022586107254028), ('woman', 0.7829256057739258), ('guy', 0.7739418745040894), ('boy', 0.7470531463623047), ('baby', 0.7217075228691101), ('man', 0.7129091024398804), ('dog', 0.7049890160560608), ('lady', 0.6941564679145813), ('bit', 0.6912130117416382), ('helper', 0.6724315881729126)]\n",
      "\n",
      "\n",
      "South Park model: \n",
      "\n",
      "[('woman', 0.8058767318725586), ('man', 0.7665978074073792), ('penis', 0.757237434387207), ('lady', 0.7549633979797363), ('boy', 0.736825704574585), ('dog', 0.7346210479736328), ('vagina', 0.6926639080047607), ('child', 0.6892564296722412), ('little', 0.674797773361206), ('heart', 0.6698172092437744)]\n"
     ]
    }
   ],
   "source": [
    "def print_similar_words(word):\n",
    "    print(\"Similar words for the word: \" + word + \"\\n\")\n",
    "    print(\"Futurama model: \\n\" + \"\")\n",
    "    print(futur_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"American Dad model: \\n\")\n",
    "    print(amDad_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"Family Guy model: \\n\")\n",
    "    print(famGuy_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"Simpsons model: \\n\")\n",
    "    print(simp_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"South Park model: \\n\")\n",
    "    print(sp_model.wv.most_similar(word, topn=10))\n",
    "print_similar_words(\"girl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for the word: science\n",
      "\n",
      "Futurama model: \n",
      "\n",
      "[('cards', 0.9975329041481018), ('fall', 0.9974583387374878), ('rescue', 0.9970595836639404), ('penny', 0.9970381855964661), ('respect', 0.9968644976615906), ('bath', 0.9966974258422852), ('lover', 0.9966610670089722), ('animals', 0.9965253472328186), ('minds', 0.9964526891708374), ('making', 0.9964302778244019)]\n",
      "\n",
      "\n",
      "American Dad model: \n",
      "\n",
      "[('kit', 0.9689302444458008), ('annual', 0.9665029048919678), ('glory', 0.963065505027771), ('spare', 0.9608169198036194), ('level', 0.9607637524604797), ('ending', 0.9607020616531372), ('fitness', 0.9603885412216187), ('pie', 0.9584792852401733), ('commitment', 0.9575470685958862), ('mount', 0.9571338295936584)]\n",
      "\n",
      "\n",
      "Family Guy model: \n",
      "\n",
      "[('papers', 0.8639549612998962), ('fecal', 0.8612370491027832), ('jeep', 0.8589364290237427), ('hippie', 0.8526805639266968), ('flu', 0.849153995513916), ('pretzel', 0.8460206985473633), ('whassat', 0.8455643653869629), ('theft', 0.8438236713409424), ('wires', 0.8422783613204956), ('comic', 0.8420206308364868)]\n",
      "\n",
      "\n",
      "Simpsons model: \n",
      "\n",
      "[('modern', 0.9151846170425415), ('service', 0.913121223449707), ('contestant', 0.9123915433883667), ('student', 0.9013375043869019), ('pride', 0.9000075459480286), ('entry', 0.8996573090553284), ('capacity', 0.8972826600074768), ('centuries', 0.8961220979690552), ('moments', 0.8938350677490234), ('haunted', 0.8938016295433044)]\n",
      "\n",
      "\n",
      "South Park model: \n",
      "\n",
      "[('ed', 0.8274605870246887), ('responsibility', 0.8165839910507202), ('summer', 0.8156164884567261), ('christian', 0.8146365880966187), ('issue', 0.8134012818336487), ('adventure', 0.8053872585296631), ('project', 0.8021315336227417), ('comedy', 0.7965900301933289), ('event', 0.7938151955604553), ('premiere', 0.7889918684959412)]\n"
     ]
    }
   ],
   "source": [
    "print_similar_words(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for the word: family\n",
      "\n",
      "Futurama model: \n",
      "\n",
      "[('surprise', 0.9983640909194946), ('national', 0.9979523420333862), ('date', 0.9979090690612793), ('liquor', 0.9978708028793335), ('died', 0.9978538751602173), ('white', 0.997847318649292), ('filthy', 0.9978205561637878), ('private', 0.9977128505706787), ('inside', 0.9976363182067871), ('third', 0.9975912570953369)]\n",
      "\n",
      "\n",
      "American Dad model: \n",
      "\n",
      "[('father', 0.8370622992515564), ('life', 0.8351610898971558), ('party', 0.8196209669113159), ('mother', 0.8146016597747803), ('hair', 0.8101903200149536), ('husband', 0.798795759677887), ('house', 0.7953490018844604), ('wife', 0.7951940894126892), ('friend', 0.7911969423294067), ('lady', 0.7898155450820923)]\n",
      "\n",
      "\n",
      "Family Guy model: \n",
      "\n",
      "[('story', 0.5983825922012329), ('dog', 0.5799961090087891), ('new', 0.5631469488143921), ('book', 0.5606899261474609), ('life', 0.5385695099830627), ('guest', 0.5376495122909546), ('body', 0.5346227288246155), ('star', 0.532819390296936), ('baby', 0.529739499092102), ('child', 0.5256866216659546)]\n",
      "\n",
      "\n",
      "Simpsons model: \n",
      "\n",
      "[('marriage', 0.7650707960128784), ('life', 0.7578208446502686), ('job', 0.746798038482666), ('moment', 0.716903805732727), ('child', 0.7024992108345032), ('father', 0.7017059922218323), ('husband', 0.7002639770507812), ('wedding', 0.6951614618301392), ('country', 0.6781333684921265), ('honor', 0.6716166734695435)]\n",
      "\n",
      "\n",
      "South Park model: \n",
      "\n",
      "[('mother', 0.7719607353210449), ('friend', 0.7554482221603394), ('father', 0.7541000843048096), ('life', 0.75143963098526), ('name', 0.721523642539978), ('child', 0.7165961265563965), ('soul', 0.7054829597473145), ('future', 0.7006587982177734), ('heart', 0.6933212876319885), ('son', 0.6909776329994202)]\n"
     ]
    }
   ],
   "source": [
    "print_similar_words(\"family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "#### Classification: *American Dad!*-*Family Guy* Model\n",
    "The model classifying *Family Guy* and *American Dad* episodes is accurate around 89% to 92% of the time. This is the lowest accuracy between the three models, reflecting both the similar writing styles and themes of the two shows. Additionally, the words most important to classifying an episode as *American Dad* are words like \"Alien\" or \"CIA\" while the words most important to classifying an episode as *Family Guy* are words such as \"white\" or \"fat\".\n",
    "\n",
    "While the two shows both center around dysfunctional families with a boisterous male patriarch, these results provide us insight to the different themes of the two TV shows. *Family Guy* is more likely to involve topics pertaining to race or comment on the weight of a character. *American Dad!*, on the other hand, has an alien as one of the main characters and uses decisions made by the CIA to move the plot of individual episodes forward. \n",
    "\n",
    "I think the biggest insight this section provides, however, is through the classification model's relative inability to differentiate between the two episodes. This parallels a theme that I have seen in commentary on animated adult comedies. After the success of *Family Guy* many shows attempted to mimic the art style and humor of the show. These similar shows include *Brickleberry*, *Paradise PD*, and *The Cleveland Show*. In the case of *American Dad*, the show was created by Seth McFarland who also created *Family Guy*, but unlike Matt Groening, the creator of *The Simpsons* and *Futurama*, he largely remained within the thematic structure that he had already set up in *Family Guy* which could be the reason why the classification model has a lower rate of accuracy. \n",
    "\n",
    "\n",
    "\n",
    "#### Classification: *Futurama*-*The Simpsons* Model\n",
    "The classification model differentiating between *Futurama* and *The Simpsons* is more accurate than the previous model, possessing a success rate of around 94%. This is due to the themes of the two shows being radically different despite their similar writing styles, spearheaded by Matt Groening and Matt Cohen. \n",
    "\n",
    "The words with the highest logistic regression weights, indicating that they are significant to the model classifying an episode as *Futurama*, are words such as \"professor\", \"robot\" and \"planet\". The words with the lowest logistic regression weights, indicating that they are significant to the model classifying an episode as *The Simpsons* are words such as \"dad\", \"kids\" and \"school\". The *Futurama* words reflect the more science fiction themes of the TV show, while *The Simpsons* words reflect the more familial themes of the show. When watching the shows, the behaviors of characters in each show are very similar. Characters such as Professor Farnsworth in *Futurama* are similar to John Frink in *The Simpsons*, and Fry's antics in *Futurama* mimic those of Bart and Homer in *The Simpsons*. Despite these similar character traits, there are nevertheless reliable thematic differences that allow the model to classify episodes reliably. \n",
    "\n",
    "An episode that is repeatedly misidentified as a *Futurama* episode is the season 23 episode \"Them, Robot\". This episode is one of the few *The Simpsons* episodes that genre bends from a family sitcom into a science-fiction narrative. After watching the episode, its themes of robotics and capitalist exploitation mirror constant language and themes in *Futurama*. This misidentification allows us to further identify what separates the two shows. That is, the genre of science-fiction divides *Futurama* from the familial sitcom genre of *The Simpsons*.\n",
    "\n",
    "#### Classification: *South Park*-*The Simpsons* Model\n",
    "The *South Park*-*Simpsons* model is by far the most accurate, and this is largely expected given they are very different shows. *South Park* is raunchier and directly satirizes political and social figures, while *The Simpsons* centers around a family and has a plot that is less directly character driven compared to *South Park*.\n",
    "\n",
    "The words with the highest logistic regression weights, indicating importance to classifying an episode as *South Park*, are words like \"dude\" or are swear words, while words with the lowest logistic regression weights are words like \"dad\", \"kids\" or \"baby. The words that are key to identifying an episode as a *Simpsons* episode, again, reflect the familial themes of the show. Even though this familial structure may not be the core plot of many episodes in the show, the familial themes of the show, nevertheless, undergird the show and its plot. \n",
    "\n",
    "An episode that is repeatedly misidentified is the season 2 episode \"Terrance and Phillip in Not Without My Anus\" which centers around Cartman finding his father. This theme of fatherhood is not present in many *South Park* episodes, but it is very reminiscent of the familial themes of *The Simpsons* which is why the classification model misinterprets the text of the episode as *The Simpsons*. Interestingly, no episodes from *The Simpsons* are repeatedly misidentified which is indicative of the rigidity of the familial and thematic structure of *The Simpsons* when compared with the diversity of themes in *South Park*\n",
    "\n",
    "\n",
    "#### Word2Vec\n",
    "The word2vec models are frequently indistinguishable when looking at the ten most similar words to words such as \"man\", but there are notable differences that can highlight the differing underlying themes and content of each show. Additionally, these differences in the way that words are used can also give insight to the subconscious decisions that writers of the TV shows make. It is worth noting that the model is different everytime it is run, so there could be differences in the similar words, but because of the size of the datasets, the output of the models are generally similar on each run. \n",
    "\n",
    "First, the word \"girl\" or the word \"woman\" has radically different similar words between the TV shows. For *Family Guy*, *American Dad* and *The Simpsons*, the similar words are all familial in nature, including words like \"kid\", but the similar words for *South Park* and *Futurama* do not include these familial themes. *South Park*'s similar words for \"girl\" include \"penis\", \"vagina\", \"turd\" and \"dog\". This is a clear outlier and gives a glimpse of a theme in the show. Women and girls are frequently only brought into the plot of a show in a sexual context. While this is likely not a conscious choice made by the writers of the show, it is nevertheless alarming and shows the subconscious decisions of the writers to limit the role of female characters in the show. *Futurama*'s similar words to \"girl\" are \"pet\", \"swamped\" and \"damned\". This is a result of Leela, the female protagonist of the show, who is a \"sewer mutant\" and constantly has her pet nearby. However, despite both Matt Groening and Matt Cohen being heavily involved with both *Futurama* and *The Simpsons*, there are clear differences in their use of female characters between the two shows. This is in contrast to the shows created by Seth McFarland, *American Dad* and *Family Guy*, which have nearly identical similar words across the board. In this case, this, again, shows the thematic range between *Futurama* and *Simpsons* when compared with the range of *American Dad* and *Family Guy*.\n",
    "\n",
    "Second, the word \"science\" also has interesting results. *South Park*'s similar words to \"science\" are \"Christian\" and \"comedy\". After seeing these words, I watched the episode \"Go God Go\" where \"science\" is frequently mentioned in its opposition to Christianity and religion. This oppositional relationship between \"science\" and religion is not mirrored in other TV shows in the dataset where \"science\" is similar to \"progress\" in the *Family Guy* model and \"modern\" or \"mysterious\" in *The Simpsons* model. \n",
    "\n",
    "Finally, the words \"dad\" and \"family\" are both highly weighted in the logistic regression for both the *Futurama*-*The Simpsons* and the *South Park*-*The Simpsons* classification models. Because of this high weighting, I decided to investigate the similar words to both \"dad\" and \"family\" in the word2vec models of all five shows in the dataset. Interestingly, every show except *Futurama* has nearly identical similar words for \"dad\" and \"family\". These similar words include \"life\", \"marriage\", \"mother\", \"child\" or \"friends\". However, *Futurama* breaks from the pack, possessing similar words like \"store\", \"date\", \"liquor\", \"crew\" and \"class\". There is again computational evidence for Matt Groening's and Matt Cohen's different writing in *Futurama* versus *The Simpsons*, but in this case, there is computational evidence for the vocabulary and themes in *Futurama* breaking from the genre of animated adult comedy TV shows as a whole. Almost every show in the genre is inundated with familial themes with a show that breaks from these themes being an exception rather than the rule. Prior to fully googling a list of shows in the genre, my dad, my brother and I thought of popular animated adult comedy TV shows that do not center around a familial unit or do not use a familial relationship to move the plot of many episodes across the length of the show's run. The only examples we could think of were *Futurama* and *BoJack Horseman*. I know many episodes of season 4 of *BoJack Horseman* center around BoJack's mother and his childhood, but compared to shows like *Archer*, the familial themes of the show are much more sporadic. Of course, this is anecdotal evidence based on three men with similar TV viewing habits, but it is nevertheless interesting that *Futurama* breaks from this convention despite Matt Groening having success with a family-based show in *The Simpsons*. After researching this further, I found an interesting [analysis](https://justtv.files.wordpress.com/2007/03/mittell_simpsons.pdf) written by Jason Mittell titled \"Cartoon Realism: Genre Mixing and the Cultural Life of *The Simpsons*\". In this paper, Mittell argues that using a familial unit as the basis for a TV comedy is relatively generic and uncontroversial in nature. This enables the TV show to be more successful especially on network television because of its much wider appeal. The deeper investigation that word2vec enabled highlights the usefulness of word2vec in enabling close reading and deeper investigation of themes that may be overlooked when watching a show. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project adds to the ongoing discussion about adult animated comedy shows, hoping to provide answers about the distinctions between the most popular shows in the genre over the last three decades. However, there are limitations to the analysis presented. First, as I discussed in the data section above, there are a few shows including *Archer* and *King of the Hill* that are not included in the dataset due to copyright issues filed by the shows' production studios. Second, if a show is currently still releasing new episodes, the most recent season of a TV show does not have available transcripts. For example, \"The Pandemic Special\" of *South Park* does not have an available transcript because it was released recently. \n",
    "\n",
    "Going forward, analyzing the results of Matt Groening's and Seth McFarland's writing in different shows was intriguing, and I would like to see how the same comedy writers change over time or write differently when writing for two shows that are running at the same time. Additionally, similar analyses could be done for other TV show genres or subsections of TV show genres. For example, analysis could be done on the difference in writing between multi-cam sitcoms such as *Big Bang Theory* and *2 Broke Girls* and single-cam sitcoms such as *Modern Family* and *Silicon Valley*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
