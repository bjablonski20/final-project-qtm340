{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiating the Humor of Long-Running Animated Adult Comedy TV Shows\n",
    "#### Ben Jablonski\n",
    "#### 11-28-2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "The raw code for this notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "Since the emergence of network television, comedy has been a perennial TV show genre, but there is an incredible variety between the writing and themes of different TV shows. Specifically, following the success of *The Simpsons* in the early 1990s, animated adult comedy shows gained a foothold in the cultural psyche. The creation of Adult Swim in 2001 created a dedicated schedule and network for adult animation, and streaming services like Netflix and Hulu have cemented funding for new animation projects. Over the last decade, adult animation has transformed from a niche genre to mainstream, with shows like *Rick and Morty* becoming cultural sensations. For a more thorough investigation of the history of adult animation, this article from [*Time*](https://time.com/5752400/adult-animation-golden-age/) is a fantastic resource. \n",
    "\n",
    "I wanted to test whether computational techniques such as classification and word2vec models can differentiate between the writing and themes of various animated comedy TV shows. Specifically, which words, themes and relationships between characters define and separate the TV shows from each other? Potentially, these methods can reveal subtextual themes that may not be clear to a casual viewer of the TV show and could elucidate choices made by the writers of TV shows whether the choices are conscious or subconscious. Because of the increasing popularity of animated comedy shows, differentiating and understanding their writing and themes serves an important cultural role through complementing the experience of watching a popular TV show. Specifically, these computational techniques allow a human viewer to further investigate previously banal relationships, and exposing these relationships can provide a critical acuity that enables a richer understanding of a TV show and the genre as a whole. \n",
    "\n",
    "\n",
    "### Relevant Studies \n",
    "There are two types of studies that are relevant to this analysis. Some studies are data based while others are more anecdotal and focus on philosophical themes of TV shows. For example, for the TV show *South Park*, there are 45 podcast episodes and 12 YouTube videos on the YouTube channel [Wisecrack](https://www.youtube.com/channel/UC6-ymYjG0SU0jUWnWh9ZzEQ) alone. These analyses fall into the second category with most of the crew of Wisecrack being philosophy graduate students or former philosophy professors. One example is the podcast episode on the \"Band in China\" *South Park* episode which discusses international copyright issues and censorship as financial blackmail from China. There are various other studies on YouTube or sites like Medium, but most of these studies focus less on the genre of animated comedy shows as a whole. Instead, they focus on individual episodes or individual TV shows. \n",
    "The other category of studies is more data science based. These studies focus on individual TV shows with the scripts of every episode. Some examples include \"[Going Down to South Park -- Text Analysis with R](https://medium.com/@vertabeloacdm/going-down-to-south-park-text-analysis-with-r-61e8beef6851)\" by Patrik Drhlik, \"[The Simpsons meets Data Visualization](https://towardsdatascience.com/the-simpsons-meets-data-visualization-ef8ef0819d13)\" by Adam Reevesman, \"[Visualizing Archer](https://medium.com/@Elijah_Meeks/visualizing-archer-bcb80e319625)\" by Elijah Meeks, or \"[Futurama: Bender's NLP](https://towardsdatascience.com/futarama-benders-nlp-775c47871ad5)\" by Isaac Kim. However, many of these studies count individual character's lines and compare the character's lines between seasons, and I find the method of simply counting character's lines to be relatively unfulfilling. For example, *The Simpsons* study above finds that Homer and Marge talk to each other the most of every character pairing, and they find that Flanders' lines have the most positive sentiment. Neither of these conclusions provide particular insight into their characters or the themes of the TV show. Another study that I wanted to specifically highlight is [\"Forecasting the Success of Television Series using Machine Learning\"](https://arxiv.org/pdf/1910.12589.pdf) by Ramya Akula, Zachary Wieselthier, Laura Martin and Ivan Garibay from the University of Central Florida. Their dataset includes mostly live action sitcoms, but I found their methods and analysis to be interesting. They use clustering and machine learning forecast models to predict the rating of an episode based on how many lines individual characters have. I found this relevant because they find statistically significant relationships between IMDb rating and a character's lines for many of the shows, but they nevertheless acknowledge the many limitations of analyzing TV show success, including varying demographics, marketing budgets and radically changing consumer preferences. I wanted to bring this sentiment into this project. The medium of television is always in flux with changing themes and writing constantly, so many conclusions about TV genres are temporally provisional. \n",
    "\n",
    "Additionally, I could not find studies that compare different animated adult comedy TV shows.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "It is difficult to be unbiased in deciding which TV shows to include in the analysis, but less than a dozen animated adult comedy shows have run for more than 5 seasons. I wanted to select TV shows with at least 60 or 70 episodes in order to have a larger amount of text to analyze for each show. Given these conditions, I decided to select the following TV shows: *South Park*, *The Simpsons*, *Futurama*, *American Dad!*, and *Family Guy*. Other shows that met the 70-episode threshold but were not selected include *Archer* and *King of the Hill*. The production studios for these shows had filed DMCA takedowns of publicly available transcripts, so they are not included. Other shows such as *Bob's Burgers* and *Aqua Teen Hunger Force* did not have complete transcripts of every episode online. \n",
    "\n",
    "There are five datasets, one for each TV show listed above, and each row of the dataset represents one line spoken in an episode of a given TV show. Therefore, a row includes the line spoken, the character speaking the line, the episode in which the line is spoken, and the season in which the episode occurs. \n",
    "\n",
    "While I have tried to fill in missing episode transcripts in order to have every episode represented in the datasets, there are some exceptions such as the *South Park* episodes \"200\" and \"201\" which have been banned for their depiction of the Islamic prophet Muhammad. Overall, these gaps in the dataset are relatively limited and do not significantly affect the accuracy of the conclusions presented. \n",
    "\n",
    "Additionally, all of these datasets can be found and downloaded on my [GitHub repo](https://github.com/bjablonski20/final-project-qtm340) along with all code used for the analysis in order to enable replication of the results below.\n",
    "\n",
    "The *Futurama*, *South Park*, and *The Simpsons* datasets were created by fans of the shows on Kaggle and GitHub, but the *Family Guy* and the *American Dad!* datasets were manually retrieved via web scraping using Beautiful Soup. \n",
    "\n",
    "The following code loads all data needed for this project from GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input",
     "hide-input",
     "hide-cell",
     "hide_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "## importing\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import little_mallet_wrapper\n",
    "import seaborn\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pandas import DataFrame\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.stats import pearsonr, norm\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "## South Park \n",
    "### retrieve csv from my github repo\n",
    "if not os.path.exists('sp_ep_data.csv.1'):\n",
    "    !wget https://raw.githubusercontent.com/bjablonski20/final-project-qtm340/main/Transcript%20CSVs/sp_ep_data.csv\n",
    "### convert into a pandas dataframe\n",
    "sp_ep_data = pd.read_csv ('sp_ep_data.csv.1',error_bad_lines=False)\n",
    "sp_ep_data = sp_ep_data.drop(['episode_link', 'season_link', 'season_name'], axis=1)\n",
    "### split dataframe into text files for each episode\n",
    "for season in range(1,sp_ep_data['season_number'].max()+1): ## season numbers\n",
    "    for episode in range(1, sp_ep_data[sp_ep_data['season_number'] == season]['season_episode_number'].max()):\n",
    "        filename = \"sp_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "        path = \"SouthPark_Data/\" + filename\n",
    "        with open(path, \"w\") as file:\n",
    "            file.writelines(sp_ep_data[(sp_ep_data['season_number'] == season) & (sp_ep_data['season_episode_number'] == episode)]['text'])\n",
    "       \n",
    "\n",
    " ##The Simpsons\n",
    "\n",
    "### retrieve csv from my github repo\n",
    "if not os.path.exists('simpsons_script_lines.csv'):\n",
    "    !wget https://raw.githubusercontent.com/bjablonski20/final-project-qtm340/main/Transcript%20CSVs/simpsons_script_lines.csv\n",
    "### convert into a pandas dataframe\n",
    "simp_ep_data = pd.read_csv ('simpsons_script_lines.csv',error_bad_lines=False)\n",
    "### Adding season column to simpsons dataset -- i know this is unwieldy but i decided to brute force it \n",
    "simp_ep_data['season'] = np.where(simp_ep_data['episode_id'] < (14),1,0)\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22))&(simp_ep_data['episode_id'] >= (14)),2,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24))&((simp_ep_data['episode_id'] >= (14+22))),3,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22))&(simp_ep_data['episode_id'] >= (14+22+24)),4,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22))&(simp_ep_data['episode_id'] >= (14+22+24+22)),5,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22)),6,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25)),7,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25)),8,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25)),9,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25)),10,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23)),11,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22)),12,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21)),13,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22)),14,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22)),15,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22)),16,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21+22))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21)),17,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401))&(simp_ep_data['episode_id'] >= (14+22+24+22+22+25+25+25+25+23+22+21+22+22+22+21+22)),18,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20))&(simp_ep_data['episode_id'] >= (401)),19,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21))&(simp_ep_data['episode_id'] >= (401+20)),20,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23))&(simp_ep_data['episode_id'] >= (401+20+21)),21,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22))&(simp_ep_data['episode_id'] >= (401+20+21+23)),22,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22+22))&(simp_ep_data['episode_id'] >= (401+20+21+23+22)),23,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22+22+22))&(simp_ep_data['episode_id'] >= (401+20+21+23+22+22)),24,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] < (401+20+21+23+22+22+22+22))&(simp_ep_data['episode_id'] >= (401+20+21+23+22+22+22)),25,simp_ep_data['season'])\n",
    "simp_ep_data['season'] = np.where((simp_ep_data['episode_id'] >= (401+20+21+23+22+22+22+22)),26,simp_ep_data['season'])\n",
    "### Split dataframe into individual text files for each episode\n",
    "for season in range(1,simp_ep_data['season'].max()+1): ## season numbers\n",
    "    if type(simp_ep_data[simp_ep_data['season'] == season]['episode_id'].max()) == type(simp_ep_data[simp_ep_data['season'] == 1]['episode_id'].max()):\n",
    "        for episode in range(int(simp_ep_data[simp_ep_data['season'] == season]['episode_id'].min()), int(simp_ep_data[simp_ep_data['season'] == season]['episode_id'].max())):\n",
    "            filename = \"simp_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "            path = \"Simpsons_Data/\" + filename\n",
    "            with open(path, \"w\") as file:\n",
    "                file.writelines(simp_ep_data[(simp_ep_data['season'] == season) & (simp_ep_data['episode_id'] == episode)]['spoken_words'])\n",
    "### these files were empty \n",
    "os.remove(\"./Simpsons_Data/simp_ep21_447_.txt\")\n",
    "os.remove(\"./Simpsons_Data/simp_ep20_424_.txt\")\n",
    "os.remove(\"./Simpsons_Data/simp_ep25_550_.txt\")\n",
    "                                  \n",
    "    \n",
    "\n",
    "\n",
    "## Futurama \n",
    "### retrieve csv from my github repo\n",
    "if not os.path.exists('futurama_ep_data.csv'):\n",
    "    !wget https://raw.githubusercontent.com/bjablonski20/final-project-qtm340/main/Transcript%20CSVs/futurama_ep_data.csv\n",
    "futur_ep_data = pd.read_csv ('futurama_ep_data.csv',error_bad_lines=False)\n",
    "\n",
    "### adds episode column\n",
    "episode = []\n",
    "ep_no = 1\n",
    "count = 0\n",
    "for index, row in futur_ep_data.iterrows(): \n",
    "    if count == 23811:\n",
    "        break\n",
    "    else:\n",
    "        if futur_ep_data['Episode'][count] != futur_ep_data['Episode'][count+1]:\n",
    "            episode.append(ep_no)\n",
    "            ep_no = ep_no +1\n",
    "            count = count +1\n",
    "        else: \n",
    "            count = count + 1\n",
    "            episode.append(ep_no)\n",
    "episode.append(114)\n",
    "futur_ep_data['Episode Number'] = episode\n",
    "\n",
    "### Splits the dataframe into individual text files for each episode \n",
    "for season in range(1,futur_ep_data['Season'].max()+1): ## season numbers\n",
    "    for episode in range(1, futur_ep_data[futur_ep_data['Season'] == season]['Episode Number'].max()):\n",
    "        filename = \"futur_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "        path = \"Futurama_Data/\" + filename\n",
    "        with open(path, \"w\") as file:\n",
    "            try:\n",
    "                file.writelines(futur_ep_data[(futur_ep_data['Season'] == season) & (futur_ep_data['Episode Number'] == episode)]['Line'])\n",
    "            except TypeError:\n",
    "                break\n",
    "### removes all empty episodes \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"Futurama_Data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(mypath + file) == 0:\n",
    "        os.remove(mypath + file)     \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Scraping \n",
    "\n",
    "### beautiful soup function that creates a pandas Dataframe with the episode, speaker and the line for the series \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re \n",
    "start = \"<b>\"\n",
    "end = \"</b>\"\n",
    "def grab_urls(soup):\n",
    "  episode_urls = []\n",
    "  for season in soup.findAll('div', style=\"column-count:2\"):\n",
    "    for episode in season.findAll('a'):\n",
    "        try:\n",
    "           episode_urls.append(episode.attrs['href'])\n",
    "        except:\n",
    "           continue\n",
    "  return episode_urls\n",
    "def scripts_from_html(html):\n",
    "    html_page = requests.get(html, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(html_page.content, 'html.parser')\n",
    "    urls = grab_urls(soup)\n",
    "    texts = []\n",
    "    characters = []\n",
    "    episode = []\n",
    "    ep_no = 1\n",
    "    character = \"NONE\"\n",
    "    for i in urls:\n",
    "        html = \"https://transcripts.fandom.com/\" + i\n",
    "        count = 0\n",
    "        html_page = requests.get(html)\n",
    "        html_string = html_page.text\n",
    "        soup = BeautifulSoup(html_string, 'html.parser')\n",
    "        for i in soup.find_all(\"p\"):\n",
    "            text1 = str(i)\n",
    "            if \"<p>\" in text1:\n",
    "                text1 = text1[text1.index(\"<p>\")+len(\"<p>\"):text1.index(\"</p>\")]\n",
    "            episode.append(ep_no)\n",
    "            if \"</b>\" in str(i):\n",
    "                text2 = str(i)\n",
    "                character = text2[text2.index(\"<b>\")+len(\"<b>\"):text2.index(\"</b>\")]\n",
    "            characters.append(character)\n",
    "            count = count + 1\n",
    "            if \"</b>\" in text1:\n",
    "                text1 = text1[text1.index(\"</b>\")+len(\"</b>\"):len(text1)]\n",
    "            texts.append(text1)\n",
    "        ep_no = ep_no + 1\n",
    "\n",
    "    d = {'Character': characters,'Episode': episode, 'Line': texts}\n",
    "    return pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## American Dad\n",
    "html_amDad = \"https://transcripts.fandom.com/wiki/American_Dad!\"\n",
    "amDad_ep_data = scripts_from_html(html_amDad)\n",
    "## adds the season variable -- done manually because i couldnt find season number as a header when parsing through the html \n",
    "amDad_ep_data['season'] = np.where(amDad_ep_data['Episode'] < (23),1,0)\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19))&(amDad_ep_data['Episode'] >= (23)),2,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16))&(amDad_ep_data['Episode'] >= (23+19)),3,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20))&(amDad_ep_data['Episode'] >= (23+19+16)),4,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20+18))&(amDad_ep_data['Episode'] >= (23+19+16+20)),5,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20+18+19))&(amDad_ep_data['Episode'] >= (23+19+16+20+18)),6,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (23+19+16+20+18+19+18))&(amDad_ep_data['Episode'] >= (23+19+16+20+18+19)),7,amDad_ep_data['season'])\n",
    "\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19))&(amDad_ep_data['Episode'] >= (133)),8,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20))&(amDad_ep_data['Episode'] >= (133+19)),9,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18))&(amDad_ep_data['Episode'] >= (133+19+20)),10,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22))&(amDad_ep_data['Episode'] >= (133+19+20+18)),11,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22+22))&(amDad_ep_data['Episode'] >= (133+19+20+18+22)),12,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22+22+22))&(amDad_ep_data['Episode'] >= (133+19+20+18+22+22+22)),13,amDad_ep_data['season'])\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (133+19+20+18+22+22+22+22))&(amDad_ep_data['Episode'] >= (133+19+20+18+22+22+22+22)),14,amDad_ep_data['season'])\n",
    "\n",
    "amDad_ep_data['season'] = np.where((amDad_ep_data['Episode'] < (278+22))&(amDad_ep_data['Episode'] >= (278)),15,amDad_ep_data['season'])\n",
    "\n",
    "## create individual .txt docs for each episode\n",
    "for season in range(1,amDad_ep_data['season'].max()+1): ## season numbers\n",
    "    try:\n",
    "        for episode in range(1, (int)(amDad_ep_data[amDad_ep_data['season'] == season]['Episode'].max())):\n",
    "            filename = \"amDad_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "            path = \"amDad_Data/\" + filename\n",
    "            with open(path, \"w\") as file:\n",
    "                file.writelines(amDad_ep_data[(amDad_ep_data['season'] == season) & (amDad_ep_data['Episode'] == episode)]['Line'])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "\n",
    "## removes all empty episodes \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"amDad_Data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(mypath + file) == 0:\n",
    "        os.remove(mypath + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Family Guy\n",
    "html_famGuy = \"https://transcripts.fandom.com/wiki/Family_Guy\"\n",
    "famGuy_ep_data = scripts_from_html(html_famGuy)\n",
    "famGuy_ep_data['season'] = np.where(famGuy_ep_data['Episode'] < (8),1,0)\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21))&(famGuy_ep_data['Episode'] >= (8)),2,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22))&(famGuy_ep_data['Episode'] >= (8+21)),3,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30))&(famGuy_ep_data['Episode'] >= (8+21+22)),4,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30+18))&(famGuy_ep_data['Episode'] >= (8+21+22+30)),5,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30+18+12))&(famGuy_ep_data['Episode'] >= (8+21+22+30+18)),6,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (8+21+22+30+18+12+16))&(famGuy_ep_data['Episode'] >= (8+21+22+30+18+12)),7,famGuy_ep_data['season'])\n",
    "\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21))&(famGuy_ep_data['Episode'] >= (112)),8,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18))&(famGuy_ep_data['Episode'] >= (112+21)),9,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23))&(famGuy_ep_data['Episode'] >= (112+21+18)),10,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22))&(famGuy_ep_data['Episode'] >= (112+21+18+23)),11,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22+21))&(famGuy_ep_data['Episode'] >= (112+21+18+23+22)),12,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22+21+18))&(famGuy_ep_data['Episode'] >= (112+21+18+23+22+21)),13,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (112+21+18+23+22+21+18+20))&(famGuy_ep_data['Episode'] >= (112+21+18+23+22+21+18)),14,famGuy_ep_data['season'])\n",
    "\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20))&(famGuy_ep_data['Episode'] >= (255)),15,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20))&(famGuy_ep_data['Episode'] >= (255+20)),16,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20+20))&(famGuy_ep_data['Episode'] >= (255+20+20)),17,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20+20+20))&(famGuy_ep_data['Episode'] >= (255+20+20+20)),18,famGuy_ep_data['season'])\n",
    "famGuy_ep_data['season'] = np.where((famGuy_ep_data['Episode'] < (255+20+20+20+20+22))&(famGuy_ep_data['Episode'] >= (255+20+20+20+20)),19,famGuy_ep_data['season'])\n",
    "\n",
    "\n",
    "## creates individual .txt files for each episode\n",
    "for season in range(1,famGuy_ep_data['season'].max()+1): ## season numbers\n",
    "    try:\n",
    "        for episode in range(1, (int)(famGuy_ep_data[famGuy_ep_data['season'] == season]['Episode'].max())):\n",
    "            filename = \"famGuy_ep\" + str(season) + \"_\" + str(episode)+\"_.txt\"\n",
    "            path = \"famGuy_Data/\" + filename\n",
    "            with open(path, \"w\") as file:\n",
    "                file.writelines(famGuy_ep_data[(famGuy_ep_data['season'] == season) & (famGuy_ep_data['Episode'] == episode)]['Line'])\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "\n",
    "## removes empty files \n",
    "mypath = \"famGuy_Data/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "for file in onlyfiles:\n",
    "    if os.path.getsize(mypath + file) == 0:\n",
    "        os.remove(mypath + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Method\n",
    "I created three different classification models. The models differentiate between *American Dad!* and *Family Guy* episodes, *The Simpsons* and *Futurama* episodes, and *South Park* and *The Simpsons* episodes. The first two pairs are selected because the shows in each pair are created and written by many of the same people while the last pair is picked because *South Park* and *The Simpsons* are the two longest running shows in the dataset. Additionally, stop words are taken out of the text, including show specific place and character names. The stop words used for this analysis can also be downloaded on my GitHub repo. \n",
    "\n",
    "The goal of this section is both to test whether the classification method can differentiate between similar TV shows based only on the text of an episode of the show and also to find which words are most important in differentiating between any two TV shows. The relative ability of the classification model to distinguish between episodes of specific shows can provide an indicator of the similarity of two shows which is important to identify the diversity of humor and theme between two shows. Additionally, identifying which episodes the model repeatedly misidentifies provides opportunity for close reading by looking at the text of the show and identifying *why* the classification model has failed. The logistic regression weights given to individual words indicate the importance of words to the identification of a show. While it is an imperfect measure, the highly weighted words can potentially paint an overall theme that differentiates a show from its counterpart in the model. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Method \n",
    "I created five separate Word2Vec models with each model being trained on every script of an individual TV show. This technique enables parsing out vocabulary and thematic differences between TV shows by investigating the similar words in one model compared to another, and it additionally provides a show's provisional definition of a word by displaying which words are similar in the vocabulary of that show. \n",
    "\n",
    "While the classification section attempts to define and differentiate shows on a binary basis (i.e. What themes and words are uniquely characteristic of *The Simpsons* rather than *Futurama*?), this section primarily seeks to find unique differences in the vocabulary of an individual TV show in relation to the four other shows in the dataset. This is, by nature, much more open ended, seeking to identify major themes that can uniquely characterize an individual TV show. \n",
    "\n",
    "The following code trains all five word2vec models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing packages\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "## South Park\n",
    "base_dir = \"./SouthPark_Data/\" \n",
    "\n",
    "all_docs = [] \n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# Get titles\n",
    "directory = \"./SouthPark_Data/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "sp_titles = [Path(file).stem for file in files]\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "       # print(sp_titles[counter]) # let's print the title of the obit\n",
    "       # print(len(sentences))  # let's check how many sentences there are per obit\n",
    "        #print(\"\\n\")\n",
    "        counter += 1\n",
    "    return all_txt\n",
    "\n",
    "sentences = make_sentences(all_docs)\n",
    "\n",
    "# trains our model!\n",
    "sp_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, \n",
    "    size=200,\n",
    "    workers=5) # parallel processing; \n",
    "sp_model.save('sp_model') ## saves the model\n",
    "\n",
    "## Simpsons\n",
    "\n",
    "\n",
    "base_dir = \"./Simpsons_Data/\" \n",
    "\n",
    "all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# need our handy nltk tokenizer \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and we'll get titles\n",
    "directory = \"./Simpsons_Data/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "simp_titles = [Path(file).stem for file in files]\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "        #print(simp_titles[counter]) # let's print the title of the obit\n",
    "       # print(len(sentences))  # let's check how many sentences there are per obit\n",
    "       # print(\"\\n\")\n",
    "        counter += 1\n",
    "    return all_txt\n",
    "\n",
    "sentences = make_sentences(all_docs)\n",
    "\n",
    "# let's train our model!\n",
    "simp_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "simp_model.save('simp_model')\n",
    "\n",
    "\n",
    "\n",
    "## Family Guy\n",
    "base_dir = \"./famGuy_Data/\" \n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "    return all_txt\n",
    "def createW2VSentences(dire):\n",
    "    all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "    docs = os.listdir(dire) # get a list of all the files in the directory\n",
    "\n",
    "    for doc in docs: # iterate through the docs\n",
    "        if not doc.startswith('.'): # get only the .txt files\n",
    "            with open(dire + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "                text = file.read() # read in the file as a single text string\n",
    "                all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# need our handy nltk tokenizer \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and we'll get titles\n",
    "    files = glob.glob(f\"{dire}/*.txt\")\n",
    "    par_titles = [Path(file).stem for file in files]\n",
    "\n",
    "    sentences = make_sentences(all_docs)\n",
    "    return sentences\n",
    "\n",
    "# let's train our model!\n",
    "famGuyDir = \"./famGuy_Data/\"\n",
    "sentences = createW2VSentences(famGuyDir)\n",
    "\n",
    "famGuy_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "famGuy_model.save('famGuy_model')\n",
    "\n",
    "## American Dad\n",
    "amDadDir = \"./amDad_Data/\"\n",
    "sentences = createW2VSentences(amDadDir)\n",
    "\n",
    "amDad_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "amDad_model.save('amDad_model')\n",
    "\n",
    "## Futurama\n",
    "futurDir = \"./Futurama_Data/\"\n",
    "sentences = createW2VSentences(futurDir)\n",
    "\n",
    "futur_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=5, # default is 5; this trims the corpus for words only used once; \n",
    "    size=200, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython\n",
    "futur_model.save('futur_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "#### Classification: *American Dad!*-*Family Guy* Model\n",
    "The model classifying *Family Guy* and *American Dad* episodes is accurate around 89% to 92% of the time. This is the lowest accuracy between the three models, reflecting both the similar writing styles and themes of the two shows. Additionally, the words most important to classifying an episode as *American Dad* are words like \"Alien\", \"USA\" or \"CIA\" while words such as \"white\" or \"fat\" are most important to classifying an episode as *Family Guy*.\n",
    "\n",
    "While the two shows both center around dysfunctional families with boisterous male patriarchs, these results provide us insight to the different themes of the two TV shows. *Family Guy* is more likely to involve topics pertaining to race or comment on the weight of a character. *American Dad!*, on the other hand, has an alien as one of the main characters and uses decisions made by the CIA to move the plot of individual episodes forward. \n",
    "\n",
    "I think the biggest insight this section provides, however, is through the classification model's relative inability to differentiate between the two episodes. This parallels a theme that I have seen in commentary on animated adult comedies. After the success of *Family Guy* many shows attempted to mimic the art style and humor of the show. These similar shows include *Brickleberry*, *Paradise PD*, and *The Cleveland Show*. In the case of *American Dad*, the show was created by Seth McFarland who also created *Family Guy*, but unlike Matt Groening, the creator of *The Simpsons* and *Futurama*, he largely remained within the thematic structure that he had already set up in *Family Guy* which could be the reason why the classification model has a lower rate of accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Family Guy-American Dad\n",
    "### General Things \n",
    "### load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('./docs/stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "### create dtm\n",
    "corpus_path = './Classification_Data_AmDadFamGuy/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')\n",
    "\n",
    "\n",
    "directory = \"./Classification_Data_AmDadFamGuy/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "titles = [Path(file).stem for file in files]\n",
    "\n",
    "corpus = []\n",
    "for title in titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "fulltext = []\n",
    "for filepath in glob.iglob('Classification_Data_AmDadFamGuy/*.txt'): ## grab titles\n",
    "    filepaths.append(filepath)\n",
    "for filepath in filepaths: ## grab text and get the scores of each individual episode\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        fulltext.append(text)\n",
    "clasdict = {'Name': filepaths, 'Text': fulltext\n",
    "}\n",
    "classdf = pd.DataFrame(clasdict)\n",
    "def changeFilePath(filePath):\n",
    "    newPath = filePath[32:]\n",
    "    return newPath\n",
    "classdf['Name'] = classdf['Name'].apply(changeFilePath)\n",
    "amDadBool = []\n",
    "\n",
    "for name in classdf['Name']:\n",
    "    if name[0:4] == \"amDa\":\n",
    "        amDadBool.append(1)\n",
    "    else:\n",
    "        amDadBool.append(0)\n",
    "classdf['amDad'] = amDadBool\n",
    "\n",
    "df_concat = pd.concat([classdf, df], axis = 1)\n",
    "noAD = df_concat['amDad'].sum()\n",
    "noAD = int(noAD)\n",
    "df_simp = df_concat[df_concat['amDad'] == 0]\n",
    "df_sp = df_concat[df_concat['amDad'] == 1]\n",
    "df_simp = df_simp.sample(n=noAD)\n",
    "df_final = pd.concat([df_simp, df_sp])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "meta = df_final[[\"Name\", \"amDad\"]]\n",
    "df = df_final.loc[:,'000':]\n",
    "\n",
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''\n",
    "\n",
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')\n",
    "\n",
    "\n",
    "for this_index in df_final.index.tolist():\n",
    "    title = meta.loc[meta.index[this_index], 'Name'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'amDad']\n",
    "    #print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'amDad'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "sum_column = meta['amDad'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "\n",
    "sum_column = meta['amDad'] - meta['PREDICTED']\n",
    "meta['Result'] = sum_column\n",
    "\n",
    "\n",
    "meta[meta['RESULT'] == 1]\n",
    "\n",
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['amDad']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['amDad']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['amDad']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "feat_df = feat_pval_weight(meta, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Family Guy-American Dad model is 0.9023255813953488\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(meta[meta['RESULT'] == 0])/len(meta)\n",
    "print(\"The accuracy of the Family Guy-American Dad model is \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3764</th>\n",
       "      <td>usa</td>\n",
       "      <td>3.490741e-33</td>\n",
       "      <td>-1.366430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3219</th>\n",
       "      <td>save</td>\n",
       "      <td>3.435987e-05</td>\n",
       "      <td>-0.432497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>straight</td>\n",
       "      <td>9.139758e-03</td>\n",
       "      <td>-0.432255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3720</th>\n",
       "      <td>true</td>\n",
       "      <td>1.286436e-02</td>\n",
       "      <td>-0.428520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>sky</td>\n",
       "      <td>4.786872e-33</td>\n",
       "      <td>-0.413460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>shot</td>\n",
       "      <td>9.190419e-01</td>\n",
       "      <td>-0.396616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>cia</td>\n",
       "      <td>9.839928e-11</td>\n",
       "      <td>-0.375171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>need</td>\n",
       "      <td>6.653096e-05</td>\n",
       "      <td>-0.371639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>hmm</td>\n",
       "      <td>3.276732e-04</td>\n",
       "      <td>-0.340708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3863</th>\n",
       "      <td>whoo</td>\n",
       "      <td>4.405436e-03</td>\n",
       "      <td>-0.336842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>buy</td>\n",
       "      <td>7.593209e-02</td>\n",
       "      <td>-0.329047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>race</td>\n",
       "      <td>6.411746e-16</td>\n",
       "      <td>-0.306110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1141</th>\n",
       "      <td>alien</td>\n",
       "      <td>4.273768e-04</td>\n",
       "      <td>-0.284568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>trying</td>\n",
       "      <td>6.096096e-02</td>\n",
       "      <td>-0.252367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>babe</td>\n",
       "      <td>2.821333e-04</td>\n",
       "      <td>-0.243975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2634</th>\n",
       "      <td>mama</td>\n",
       "      <td>6.611083e-03</td>\n",
       "      <td>-0.242355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>didn</td>\n",
       "      <td>2.961122e-02</td>\n",
       "      <td>-0.202194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487</th>\n",
       "      <td>knew</td>\n",
       "      <td>3.168798e-02</td>\n",
       "      <td>-0.190989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1904</th>\n",
       "      <td>end</td>\n",
       "      <td>2.057200e-01</td>\n",
       "      <td>-0.179324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>american</td>\n",
       "      <td>1.992455e-07</td>\n",
       "      <td>-0.160404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT       P_VALUE  LR_WEIGHT\n",
       "3764       usa  3.490741e-33  -1.366430\n",
       "3219      save  3.435987e-05  -0.432497\n",
       "3519  straight  9.139758e-03  -0.432255\n",
       "3720      true  1.286436e-02  -0.428520\n",
       "3363       sky  4.786872e-33  -0.413460\n",
       "3312      shot  9.190419e-01  -0.396616\n",
       "1547       cia  9.839928e-11  -0.375171\n",
       "2761      need  6.653096e-05  -0.371639\n",
       "2318       hmm  3.276732e-04  -0.340708\n",
       "3863      whoo  4.405436e-03  -0.336842\n",
       "1430       buy  7.593209e-02  -0.329047\n",
       "3076      race  6.411746e-16  -0.306110\n",
       "1141     alien  4.273768e-04  -0.284568\n",
       "3725    trying  6.096096e-02  -0.252367\n",
       "1223      babe  2.821333e-04  -0.243975\n",
       "2634      mama  6.611083e-03  -0.242355\n",
       "1770      didn  2.961122e-02  -0.202194\n",
       "2487      knew  3.168798e-02  -0.190989\n",
       "1904       end  2.057200e-01  -0.179324\n",
       "1148  american  1.992455e-07  -0.160404"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>old</td>\n",
       "      <td>1.013702e-06</td>\n",
       "      <td>0.162016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>crap</td>\n",
       "      <td>1.738423e-18</td>\n",
       "      <td>0.167223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>brought</td>\n",
       "      <td>4.178643e-03</td>\n",
       "      <td>0.176746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2345</th>\n",
       "      <td>horse</td>\n",
       "      <td>6.421214e-01</td>\n",
       "      <td>0.194281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>huh</td>\n",
       "      <td>2.723837e-08</td>\n",
       "      <td>0.195783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>dancing</td>\n",
       "      <td>2.645358e-01</td>\n",
       "      <td>0.196719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>tucker</td>\n",
       "      <td>2.309205e-08</td>\n",
       "      <td>0.205411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>children</td>\n",
       "      <td>3.521932e-04</td>\n",
       "      <td>0.212781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159</th>\n",
       "      <td>right</td>\n",
       "      <td>2.552637e-26</td>\n",
       "      <td>0.231068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>guys</td>\n",
       "      <td>9.218887e-14</td>\n",
       "      <td>0.239201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3074</th>\n",
       "      <td>quite</td>\n",
       "      <td>5.671996e-06</td>\n",
       "      <td>0.262618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3778</th>\n",
       "      <td>video</td>\n",
       "      <td>2.988741e-02</td>\n",
       "      <td>0.288131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2477</th>\n",
       "      <td>kind</td>\n",
       "      <td>2.168777e-12</td>\n",
       "      <td>0.301849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>everybody</td>\n",
       "      <td>3.589498e-12</td>\n",
       "      <td>0.330784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3640</th>\n",
       "      <td>theme</td>\n",
       "      <td>9.089200e-04</td>\n",
       "      <td>0.366427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>fat</td>\n",
       "      <td>8.457820e-06</td>\n",
       "      <td>0.392243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291</th>\n",
       "      <td>hell</td>\n",
       "      <td>4.836838e-20</td>\n",
       "      <td>0.455401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>today</td>\n",
       "      <td>1.557416e-15</td>\n",
       "      <td>0.520829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2265</th>\n",
       "      <td>hat</td>\n",
       "      <td>1.361429e-06</td>\n",
       "      <td>0.587021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2511</th>\n",
       "      <td>laugh</td>\n",
       "      <td>2.582001e-23</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FEAT       P_VALUE  LR_WEIGHT\n",
       "2814        old  1.013702e-06   0.162016\n",
       "1675       crap  1.738423e-18   0.167223\n",
       "1395    brought  4.178643e-03   0.176746\n",
       "2345      horse  6.421214e-01   0.194281\n",
       "2359        huh  2.723837e-08   0.195783\n",
       "1719    dancing  2.645358e-01   0.196719\n",
       "3727     tucker  2.309205e-08   0.205411\n",
       "1528   children  3.521932e-04   0.212781\n",
       "3159      right  2.552637e-26   0.231068\n",
       "2235       guys  9.218887e-14   0.239201\n",
       "3074      quite  5.671996e-06   0.262618\n",
       "3778      video  2.988741e-02   0.288131\n",
       "2477       kind  2.168777e-12   0.301849\n",
       "1934  everybody  3.589498e-12   0.330784\n",
       "3640      theme  9.089200e-04   0.366427\n",
       "1999        fat  8.457820e-06   0.392243\n",
       "2291       hell  4.836838e-20   0.455401\n",
       "3671      today  1.557416e-15   0.520829\n",
       "2265        hat  1.361429e-06   0.587021\n",
       "2511      laugh  2.582001e-23   0.854097"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: *Futurama*-*The Simpsons* Model\n",
    "The classification model differentiating between *Futurama* and *The Simpsons* is more accurate than the previous model, possessing a success rate of around 92% to 94%. This is due to the themes of the two shows being radically different despite their similar writing styles, spearheaded by Matt Groening and Matt Cohen. \n",
    "\n",
    "The words with the highest logistic regression weights, indicating that they are significant to the model classifying an episode as *Futurama*, are words such as \"professor\", \"robot\" and \"planet\". The words with the lowest logistic regression weights, indicating that they are significant to the model classifying an episode as *The Simpsons*, are words such as \"dad\", \"kids\" and \"school\". The *Futurama* words reflect the more science-fiction themes of the TV show, while *The Simpsons* words reflect the more familial themes of the show. When watching the shows, the behaviors of characters in each show are very similar. Characters such as Professor Farnsworth in *Futurama* are similar to John Frink in *The Simpsons*, and Fry's antics in *Futurama* mimic those of Bart and Homer in *The Simpsons*. Despite these similar character traits, there are nevertheless reliable thematic differences that allow the model to classify episodes. \n",
    "\n",
    "A *Simpsons* episode that is repeatedly misidentified as a *Futurama* episode is the season 23 episode \"Them, Robot\". This episode is one of the few *The Simpsons* episodes that genre bends from a family sitcom into a science-fiction narrative. After watching the episode, its themes of robotics and capitalist exploitation mirror constant language and themes in *Futurama*. This misidentification allows us to further identify what separates the two shows. That is, the genre of science-fiction divides *Futurama* from the familial sitcom genre of *The Simpsons*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Futurama-Simpsons\n",
    "## General Things \n",
    "# load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('./docs/stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "# create dtm\n",
    "corpus_path = './Classification_Data_SimpFuturama/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')\n",
    "\n",
    "\n",
    "directory = \"./Classification_Data_SimpFuturama/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "titles = [Path(file).stem for file in files]\n",
    "\n",
    "corpus = []\n",
    "for title in titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "fulltext = []\n",
    "for filepath in glob.iglob('Classification_Data_SimpFuturama/*.txt'): ## grab titles\n",
    "    filepaths.append(filepath)\n",
    "for filepath in filepaths: ## grab text and get the scores of each individual episode\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        fulltext.append(text)\n",
    "clasdict = {'Name': filepaths, 'Text': fulltext\n",
    "}\n",
    "classdf = pd.DataFrame(clasdict)\n",
    "def changeFilePath(filePath):\n",
    "    newPath = filePath[33:]\n",
    "    return newPath\n",
    "classdf['Name'] = classdf['Name'].apply(changeFilePath)\n",
    "futramaBool = []\n",
    "\n",
    "for name in classdf['Name']:\n",
    "    if name[0:4] == \"simp\":\n",
    "        futramaBool.append(0)\n",
    "    else:\n",
    "        futramaBool.append(1)\n",
    "classdf['Futurama'] = futramaBool\n",
    "\n",
    "df_concat = pd.concat([classdf, df], axis = 1)\n",
    "noF = df_concat['Futurama'].sum()\n",
    "\n",
    "df_simp = df_concat[df_concat['Futurama'] == 0]\n",
    "df_sp = df_concat[df_concat['Futurama'] == 1]\n",
    "df_simp = df_simp.sample(n=noF)\n",
    "df_final = pd.concat([df_simp, df_sp])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "meta = df_final[[\"Name\", \"Futurama\"]]\n",
    "df = df_final.loc[:,'000':]\n",
    "\n",
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''\n",
    "\n",
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')\n",
    "\n",
    "\n",
    "for this_index in df_final.index.tolist():\n",
    "    title = meta.loc[meta.index[this_index], 'Name'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'Futurama']\n",
    "    #print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'Futurama'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "sum_column = meta['Futurama'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "\n",
    "sum_column = meta['Futurama'] - meta['PREDICTED']\n",
    "meta['Result'] = sum_column\n",
    "\n",
    "\n",
    "meta[meta['RESULT'] == 1]\n",
    "\n",
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['Futurama']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['Futurama']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['Futurama']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "feat_df = feat_pval_weight(meta, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Futurama-Simpsons model is: 0.9204545454545454\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(meta[meta['RESULT'] == 0])/len(meta)\n",
    "print(\"The accuracy of the Futurama-Simpsons model is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>professor</td>\n",
       "      <td>2.256554e-07</td>\n",
       "      <td>-0.953139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>robot</td>\n",
       "      <td>1.658894e-11</td>\n",
       "      <td>-0.822602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2109</th>\n",
       "      <td>planet</td>\n",
       "      <td>4.154998e-10</td>\n",
       "      <td>-0.650316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>delivery</td>\n",
       "      <td>6.525550e-06</td>\n",
       "      <td>-0.373202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>news</td>\n",
       "      <td>2.294563e-06</td>\n",
       "      <td>-0.313089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>alien</td>\n",
       "      <td>4.230250e-04</td>\n",
       "      <td>-0.283459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>universe</td>\n",
       "      <td>2.864122e-03</td>\n",
       "      <td>-0.266051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>captain</td>\n",
       "      <td>1.411391e-03</td>\n",
       "      <td>-0.210065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>coffee</td>\n",
       "      <td>3.491012e-01</td>\n",
       "      <td>-0.192436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>feel</td>\n",
       "      <td>1.547857e-01</td>\n",
       "      <td>-0.173120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3067</th>\n",
       "      <td>warning</td>\n",
       "      <td>5.028199e-01</td>\n",
       "      <td>-0.169993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>food</td>\n",
       "      <td>4.550013e-01</td>\n",
       "      <td>-0.167103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2724</th>\n",
       "      <td>stole</td>\n",
       "      <td>2.927528e-01</td>\n",
       "      <td>-0.150188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2515</th>\n",
       "      <td>ship</td>\n",
       "      <td>1.219713e-04</td>\n",
       "      <td>-0.147836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606</th>\n",
       "      <td>land</td>\n",
       "      <td>4.892819e-01</td>\n",
       "      <td>-0.136781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2792</th>\n",
       "      <td>surface</td>\n",
       "      <td>3.976434e-02</td>\n",
       "      <td>-0.120579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>ass</td>\n",
       "      <td>3.941384e-06</td>\n",
       "      <td>-0.117114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2874</th>\n",
       "      <td>things</td>\n",
       "      <td>6.491574e-02</td>\n",
       "      <td>-0.111013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>wanna</td>\n",
       "      <td>5.629945e-04</td>\n",
       "      <td>-0.104521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>654</th>\n",
       "      <td>course</td>\n",
       "      <td>1.883069e-02</td>\n",
       "      <td>-0.102016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           FEAT       P_VALUE  LR_WEIGHT\n",
       "2209  professor  2.256554e-07  -0.953139\n",
       "2365      robot  1.658894e-11  -0.822602\n",
       "2109     planet  4.154998e-10  -0.650316\n",
       "757    delivery  6.525550e-06  -0.373202\n",
       "1915       news  2.294563e-06  -0.313089\n",
       "65        alien  4.230250e-04  -0.283459\n",
       "3006   universe  2.864122e-03  -0.266051\n",
       "413     captain  1.411391e-03  -0.210065\n",
       "559      coffee  3.491012e-01  -0.192436\n",
       "1064       feel  1.547857e-01  -0.173120\n",
       "3067    warning  5.028199e-01  -0.169993\n",
       "1135       food  4.550013e-01  -0.167103\n",
       "2724      stole  2.927528e-01  -0.150188\n",
       "2515       ship  1.219713e-04  -0.147836\n",
       "1606       land  4.892819e-01  -0.136781\n",
       "2792    surface  3.976434e-02  -0.120579\n",
       "127         ass  3.941384e-06  -0.117114\n",
       "2874     things  6.491574e-02  -0.111013\n",
       "3058      wanna  5.629945e-04  -0.104521\n",
       "654      course  1.883069e-02  -0.102016"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>fetch</td>\n",
       "      <td>2.623289e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>festival</td>\n",
       "      <td>9.725443e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>female</td>\n",
       "      <td>2.678444e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>gonna</td>\n",
       "      <td>1.227304e-03</td>\n",
       "      <td>0.008175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>night</td>\n",
       "      <td>5.719649e-02</td>\n",
       "      <td>0.010504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>la</td>\n",
       "      <td>6.857983e-02</td>\n",
       "      <td>0.014020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>sir</td>\n",
       "      <td>1.742952e-01</td>\n",
       "      <td>0.014594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>book</td>\n",
       "      <td>1.331759e-04</td>\n",
       "      <td>0.017958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>job</td>\n",
       "      <td>3.744752e-01</td>\n",
       "      <td>0.022288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>baseball</td>\n",
       "      <td>1.720918e-01</td>\n",
       "      <td>0.038820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2346</th>\n",
       "      <td>reverend</td>\n",
       "      <td>2.317060e-02</td>\n",
       "      <td>0.055555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>mother</td>\n",
       "      <td>9.310922e-03</td>\n",
       "      <td>0.059787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2354</th>\n",
       "      <td>right</td>\n",
       "      <td>1.729000e-09</td>\n",
       "      <td>0.124352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>family</td>\n",
       "      <td>1.697803e-06</td>\n",
       "      <td>0.163840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>father</td>\n",
       "      <td>9.190236e-05</td>\n",
       "      <td>0.190074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>want</td>\n",
       "      <td>5.856496e-07</td>\n",
       "      <td>0.214354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>boy</td>\n",
       "      <td>2.824470e-06</td>\n",
       "      <td>0.288189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>town</td>\n",
       "      <td>1.606242e-06</td>\n",
       "      <td>0.310484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3170</th>\n",
       "      <td>world</td>\n",
       "      <td>6.414014e-02</td>\n",
       "      <td>0.367133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>dad</td>\n",
       "      <td>1.338554e-18</td>\n",
       "      <td>0.870315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT       P_VALUE  LR_WEIGHT\n",
       "1076     fetch  2.623289e-01   0.000000\n",
       "1075  festival  9.725443e-02   0.000000\n",
       "1074    female  2.678444e-02   0.000000\n",
       "1247     gonna  1.227304e-03   0.008175\n",
       "1921     night  5.719649e-02   0.010504\n",
       "1595        la  6.857983e-02   0.014020\n",
       "2563       sir  1.742952e-01   0.014594\n",
       "293       book  1.331759e-04   0.017958\n",
       "1534       job  3.744752e-01   0.022288\n",
       "193   baseball  1.720918e-01   0.038820\n",
       "2346  reverend  2.317060e-02   0.055555\n",
       "1860    mother  9.310922e-03   0.059787\n",
       "2354     right  1.729000e-09   0.124352\n",
       "1034    family  1.697803e-06   0.163840\n",
       "1051    father  9.190236e-05   0.190074\n",
       "3059      want  5.856496e-07   0.214354\n",
       "318        boy  2.824470e-06   0.288189\n",
       "2928      town  1.606242e-06   0.310484\n",
       "3170     world  6.414014e-02   0.367133\n",
       "713        dad  1.338554e-18   0.870315"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: *South Park*-*The Simpsons* Model\n",
    "The *South Park*-*Simpsons* model is by far the most accurate, and this is largely expected given they are very different shows. *South Park* is raunchier and directly satirizes political and social figures, while *The Simpsons* centers around a family and has a plot that is less directly character driven compared to *South Park*.\n",
    "\n",
    "The words with the highest logistic regression weights, indicating importance to classifying an episode as *South Park*, are words like \"dude\" or are swear words, while words with the lowest logistic regression weights are words like \"dad\", \"kids\" or \"baby\". The words that are key to identifying an episode as a *Simpsons* episode, again, reflect the familial themes of the show. Even though this familial structure may not be the core plot of many episodes in the show, the familial themes of the show, nevertheless, undergird the show and its plot. \n",
    "\n",
    "A *South Park* episode that is repeatedly misidentified is the season 2 episode \"Terrance and Phillip in Not Without My Anus\" which centers around Cartman finding his father. This theme of fatherhood is not present in many *South Park* episodes, but it is very reminiscent of the familial themes of *The Simpsons* which is why the classification model misinterprets the text of the episode as *The Simpsons*. Interestingly, no episodes from *The Simpsons* are repeatedly misidentified which is indicative of the rigidity of the familial and thematic structure of *The Simpsons* when compared with the diversity of themes in *South Park*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## South Park-Simpsons\n",
    "## General Things \n",
    "# load stopwords\n",
    "from sklearn.feature_extraction import text\n",
    "text_file = open('./docs/stopwords.txt')\n",
    "jockers_words = text_file.read().split()\n",
    "new_stopwords = text.ENGLISH_STOP_WORDS.union(jockers_words)\n",
    "\n",
    "# create dtm\n",
    "corpus_path = './Classification_Data/'\n",
    "vectorizer = CountVectorizer(input='filename', encoding='utf8', stop_words = new_stopwords, min_df=20, dtype='float64')\n",
    "\n",
    "\n",
    "directory = \"./Classification_Data/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "titles = [Path(file).stem for file in files]\n",
    "\n",
    "corpus = []\n",
    "for title in titles:\n",
    "    filename = title + \".txt\"\n",
    "    corpus.append(corpus_path + filename)\n",
    "dtm = vectorizer.fit_transform(corpus)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "matrix = dtm.toarray()\n",
    "df = DataFrame(matrix, columns=vocab)\n",
    "\n",
    "\n",
    "filepaths = []\n",
    "fulltext = []\n",
    "for filepath in glob.iglob('Classification_Data/*.txt'): ## grab titles\n",
    "    filepaths.append(filepath)\n",
    "for filepath in filepaths: ## grab text and get the scores of each individual episode\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "        fulltext.append(text)\n",
    "clasdict = {'Name': filepaths, 'Text': fulltext\n",
    "}\n",
    "classdf = pd.DataFrame(clasdict)\n",
    "def changeFilePath(filePath):\n",
    "    newPath = filePath[20:]\n",
    "    return newPath\n",
    "classdf['Name'] = classdf['Name'].apply(changeFilePath)\n",
    "southParkBool = []\n",
    "\n",
    "for name in classdf['Name']:\n",
    "    if name[0:4] == \"simp\":\n",
    "        southParkBool.append(0)\n",
    "    else:\n",
    "        southParkBool.append(1)\n",
    "classdf['SouthPark'] = southParkBool\n",
    "\n",
    "df_concat = pd.concat([classdf, df], axis = 1)\n",
    "noSP = df_concat['SouthPark'].sum()\n",
    "\n",
    "df_simp = df_concat[df_concat['SouthPark'] == 0]\n",
    "df_sp = df_concat[df_concat['SouthPark'] == 1]\n",
    "df_simp = df_simp.sample(n=noSP)\n",
    "df_final = pd.concat([df_simp, df_sp])\n",
    "df_final = df_final.reset_index()\n",
    "df_final = df_final.drop(columns=\"index\")\n",
    "meta = df_final[[\"Name\", \"SouthPark\"]]\n",
    "df = df_final.loc[:,'000':]\n",
    "\n",
    "meta['PROBS'] = ''\n",
    "meta['PREDICTED'] = ''\n",
    "\n",
    "model = LogisticRegression(penalty = 'l1', C = 1.0, solver='liblinear')\n",
    "\n",
    "\n",
    "for this_index in df_final.index.tolist():\n",
    "    title = meta.loc[meta.index[this_index], 'Name'] \n",
    "    CLASS = meta.loc[meta.index[this_index], 'SouthPark']\n",
    "    #print(title, CLASS) \n",
    "    \n",
    "    train_index_list = [index_ for index_ in df.index.tolist() if index_ != this_index] # exclude the title to be predicted\n",
    "    X = df.loc[train_index_list] # the model trains on all the data except the excluded title row\n",
    "    y = meta.loc[train_index_list, 'SouthPark'] # the y row tells the model which class each title belongs to\n",
    "    TEST_CASE = df.loc[[this_index]]\n",
    "\n",
    "    model.fit(X,y) # fit the model\n",
    "    prediction = model.predict_proba(TEST_CASE) # calculate probability of test case\n",
    "    predicted = model.predict(TEST_CASE) # calculate predicted class of test case\n",
    "    meta.at[this_index, 'PREDICTED'] = predicted # add predicted class to metadata\n",
    "    meta.at[this_index, 'PROBS'] = str(prediction) # add probabilities to metadata\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "sum_column = meta['SouthPark'] - meta['PREDICTED']\n",
    "meta['RESULT'] = sum_column\n",
    "\n",
    "\n",
    "meta = meta.replace([0], 0)\n",
    "meta = meta.replace([1], 1)\n",
    "\n",
    "sum_column = meta['SouthPark'] - meta['PREDICTED']\n",
    "meta['Result'] = sum_column\n",
    "\n",
    "\n",
    "meta[meta['RESULT'] == 1]\n",
    "\n",
    "canonic_c = 1.0\n",
    "\n",
    "def Ztest(vec1, vec2):\n",
    "\n",
    "    X1, X2 = np.mean(vec1), np.mean(vec2)\n",
    "    sd1, sd2 = np.std(vec1), np.std(vec2)\n",
    "    n1, n2 = len(vec1), len(vec2)\n",
    "\n",
    "    pooledSE = np.sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = (X1 - X2)/pooledSE\n",
    "    pval = 2*(norm.sf(abs(z)))\n",
    "\n",
    "    return z, pval\n",
    "def feat_pval_weight(meta_df_, dtm_df_):\n",
    "\n",
    "    dtm0 = dtm_df_.loc[meta_df_[meta_df_['SouthPark']==0].index.tolist()].to_numpy()\n",
    "    dtm1 = dtm_df_.loc[meta_df_[meta_df_['SouthPark']==1].index.tolist()].to_numpy()\n",
    "\n",
    "    pvals = [Ztest(dtm0[ : ,i], dtm1[ : ,i])[1] for i in range(dtm_df_.shape[1])]\n",
    "    clf = LogisticRegression(penalty = 'l1', C = canonic_c, class_weight = 'balanced', solver='liblinear')\n",
    "    clf.fit(dtm_df_, meta_df_['SouthPark']==0)\n",
    "    weights = clf.coef_[0]\n",
    "\n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    feature_df['FEAT'] = dtm_df_.columns\n",
    "    feature_df['P_VALUE'] = pvals\n",
    "    feature_df['LR_WEIGHT'] = weights\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "feat_df = feat_pval_weight(meta, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of The South Park-Simpsons Model is: 0.9849624060150376\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(meta[meta['RESULT'] == 0])/len(meta)\n",
    "print(\"The accuracy of The South Park-Simpsons Model is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>turns</td>\n",
       "      <td>6.106483e-139</td>\n",
       "      <td>-0.713890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4110</th>\n",
       "      <td>walks</td>\n",
       "      <td>7.096521e-163</td>\n",
       "      <td>-0.680366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>dude</td>\n",
       "      <td>2.348527e-111</td>\n",
       "      <td>-0.502310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3775</th>\n",
       "      <td>takes</td>\n",
       "      <td>1.800230e-90</td>\n",
       "      <td>-0.413393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>monitor</td>\n",
       "      <td>6.575918e-04</td>\n",
       "      <td>-0.277334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>away</td>\n",
       "      <td>4.399156e-117</td>\n",
       "      <td>-0.231929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>children</td>\n",
       "      <td>3.973744e-12</td>\n",
       "      <td>-0.211516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>looks</td>\n",
       "      <td>1.642480e-114</td>\n",
       "      <td>-0.171067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>ass</td>\n",
       "      <td>2.034393e-29</td>\n",
       "      <td>-0.145451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>canada</td>\n",
       "      <td>2.603544e-02</td>\n",
       "      <td>-0.131874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>guys</td>\n",
       "      <td>1.173561e-69</td>\n",
       "      <td>-0.118140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>boys</td>\n",
       "      <td>5.233973e-36</td>\n",
       "      <td>-0.108642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>gun</td>\n",
       "      <td>4.357653e-08</td>\n",
       "      <td>-0.095757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2714</th>\n",
       "      <td>people</td>\n",
       "      <td>7.226302e-46</td>\n",
       "      <td>-0.093364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>door</td>\n",
       "      <td>5.977022e-79</td>\n",
       "      <td>-0.070416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>pig</td>\n",
       "      <td>1.244586e-01</td>\n",
       "      <td>-0.069655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>bitch</td>\n",
       "      <td>9.382494e-14</td>\n",
       "      <td>-0.067401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>begins</td>\n",
       "      <td>7.905948e-75</td>\n",
       "      <td>-0.062729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2677</th>\n",
       "      <td>park</td>\n",
       "      <td>6.289981e-17</td>\n",
       "      <td>-0.062463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>grandpa</td>\n",
       "      <td>1.318293e-02</td>\n",
       "      <td>-0.055823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FEAT        P_VALUE  LR_WEIGHT\n",
       "3999     turns  6.106483e-139  -0.713890\n",
       "4110     walks  7.096521e-163  -0.680366\n",
       "1199      dude  2.348527e-111  -0.502310\n",
       "3775     takes   1.800230e-90  -0.413393\n",
       "2435   monitor   6.575918e-04  -0.277334\n",
       "230       away  4.399156e-117  -0.231929\n",
       "686   children   3.973744e-12  -0.211516\n",
       "2262     looks  1.642480e-114  -0.171067\n",
       "193        ass   2.034393e-29  -0.145451\n",
       "562     canada   2.603544e-02  -0.131874\n",
       "1750      guys   1.173561e-69  -0.118140\n",
       "446       boys   5.233973e-36  -0.108642\n",
       "1746       gun   4.357653e-08  -0.095757\n",
       "2714    people   7.226302e-46  -0.093364\n",
       "1143      door   5.977022e-79  -0.070416\n",
       "2752       pig   1.244586e-01  -0.069655\n",
       "372      bitch   9.382494e-14  -0.067401\n",
       "330     begins   7.905948e-75  -0.062729\n",
       "2677      park   6.289981e-17  -0.062463\n",
       "1702   grandpa   1.318293e-02  -0.055823"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEAT</th>\n",
       "      <th>P_VALUE</th>\n",
       "      <th>LR_WEIGHT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>big</td>\n",
       "      <td>1.047561e-06</td>\n",
       "      <td>0.087279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>sir</td>\n",
       "      <td>2.093214e-01</td>\n",
       "      <td>0.090096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>life</td>\n",
       "      <td>1.037431e-01</td>\n",
       "      <td>0.091707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4115</th>\n",
       "      <td>want</td>\n",
       "      <td>6.869435e-08</td>\n",
       "      <td>0.093827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032</th>\n",
       "      <td>real</td>\n",
       "      <td>5.184926e-03</td>\n",
       "      <td>0.102954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2258</th>\n",
       "      <td>look</td>\n",
       "      <td>9.231539e-45</td>\n",
       "      <td>0.113109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>eat</td>\n",
       "      <td>7.992704e-01</td>\n",
       "      <td>0.114671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>thanks</td>\n",
       "      <td>9.818325e-02</td>\n",
       "      <td>0.116476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>super</td>\n",
       "      <td>2.839899e-03</td>\n",
       "      <td>0.120854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>feel</td>\n",
       "      <td>1.915560e-02</td>\n",
       "      <td>0.127865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>old</td>\n",
       "      <td>1.568476e-04</td>\n",
       "      <td>0.139077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>night</td>\n",
       "      <td>5.210001e-03</td>\n",
       "      <td>0.154031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>house</td>\n",
       "      <td>3.856676e-03</td>\n",
       "      <td>0.167063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>boy</td>\n",
       "      <td>2.305208e-01</td>\n",
       "      <td>0.169447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>gotta</td>\n",
       "      <td>6.393718e-06</td>\n",
       "      <td>0.172760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>girl</td>\n",
       "      <td>2.362946e-01</td>\n",
       "      <td>0.209527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>fine</td>\n",
       "      <td>1.257663e-03</td>\n",
       "      <td>0.211653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>kids</td>\n",
       "      <td>1.892608e-10</td>\n",
       "      <td>0.226406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>day</td>\n",
       "      <td>8.742258e-02</td>\n",
       "      <td>0.249429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>dad</td>\n",
       "      <td>1.820047e-06</td>\n",
       "      <td>0.279868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FEAT       P_VALUE  LR_WEIGHT\n",
       "358      big  1.047561e-06   0.087279\n",
       "3419     sir  2.093214e-01   0.090096\n",
       "2204    life  1.037431e-01   0.091707\n",
       "4115    want  6.869435e-08   0.093827\n",
       "3032    real  5.184926e-03   0.102954\n",
       "2258    look  9.231539e-45   0.113109\n",
       "1226     eat  7.992704e-01   0.114671\n",
       "3843  thanks  9.818325e-02   0.116476\n",
       "3725   super  2.839899e-03   0.120854\n",
       "1414    feel  1.915560e-02   0.127865\n",
       "2600     old  1.568476e-04   0.139077\n",
       "2537   night  5.210001e-03   0.154031\n",
       "1922   house  3.856676e-03   0.167063\n",
       "444      boy  2.305208e-01   0.169447\n",
       "1685   gotta  6.393718e-06   0.172760\n",
       "1639    girl  2.362946e-01   0.209527\n",
       "1453    fine  1.257663e-03   0.211653\n",
       "2091    kids  1.892608e-10   0.226406\n",
       "1000     day  8.742258e-02   0.249429\n",
       "974      dad  1.820047e-06   0.279868"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_df.sort_values('LR_WEIGHT', ascending = True).tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec\n",
    "The word2vec models are frequently indistinguishable when looking at the ten most similar words to words such as \"man\", but there are notable differences that can highlight the differing underlying themes and content of each show. Additionally, these differences in the way that words are used can also give insight to the subconscious decisions that writers of the TV shows make. It is worth noting that the model is different everytime it is run, so there could be differences in the similar words, but because of the size of the datasets, the output of the models are generally similar on each run. \n",
    "\n",
    "First, the word \"girl\" or the word \"woman\" has radically different similar words between the TV shows. For *Family Guy*, *American Dad* and *The Simpsons*, the similar words are all familial in nature, including words like \"kid\", but the similar words for *South Park* and *Futurama* do not include these familial themes. *South Park*'s similar words for \"girl\" include \"penis\", \"vagina\", \"turd\" and \"dog\". This is a clear outlier and gives a glimpse of a theme in the show. Women and girls are frequently only brought into the plot of a show in a sexual context, especially in the first 10 seasons of the show. While this is likely not a conscious choice made by the writers of the show, it is nevertheless alarming and shows the subconscious decisions of the writers to limit the role of female characters in the show. *Futurama*'s similar words to \"girl\" are \"pet\", \"swamped\" and \"damned\". This is a result of Leela, the female protagonist of the show, who is a \"sewer mutant\" and constantly has her pet nearby. However, despite both Matt Groening and Matt Cohen being heavily involved with both *Futurama* and *The Simpsons*, there are clear differences in their use of female characters between the two shows. This is in contrast to the shows created by Seth McFarland, *American Dad* and *Family Guy*, which have nearly identical similar words across the board. In this case, this, again, shows the thematic range between *Futurama* and *Simpsons* when compared with the range of *American Dad* and *Family Guy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for the word: girl\n",
      "\n",
      "Futurama model: \n",
      "\n",
      "[('fake', 0.9985529184341431), ('crash', 0.9983259439468384), ('dollars', 0.9982154369354248), ('control', 0.9979572296142578), ('eating', 0.9978959560394287), ('shooting', 0.9978055357933044), ('less', 0.997783362865448), ('dig', 0.9977428317070007), ('lazy', 0.9976521134376526), ('angry', 0.9976294040679932)]\n",
      "\n",
      "\n",
      "American Dad model: \n",
      "\n",
      "[('kid', 0.8849986791610718), ('dog', 0.8093185424804688), ('alien', 0.8025319576263428), ('lady', 0.7974755167961121), ('little', 0.7966359257698059), ('second', 0.7924938201904297), ('guy', 0.7896062135696411), ('shot', 0.7758139371871948), ('person', 0.7632725238800049), ('fish', 0.762179970741272)]\n",
      "\n",
      "\n",
      "Family Guy model: \n",
      "\n",
      "[('kid', 0.8633959293365479), ('lady', 0.7928106784820557), ('dog', 0.7781652212142944), ('woman', 0.7661473751068115), ('guy', 0.7580353021621704), ('person', 0.7306180000305176), ('bit', 0.7255604863166809), ('big', 0.7163251638412476), ('joke', 0.7130960822105408), ('baby', 0.7097753286361694)]\n",
      "\n",
      "\n",
      "Simpsons model: \n",
      "\n",
      "[('kid', 0.8092536926269531), ('woman', 0.8083738088607788), ('guy', 0.7906303405761719), ('boy', 0.7394776344299316), ('bit', 0.7138171195983887), ('baby', 0.7036451101303101), ('lady', 0.7013537883758545), ('man', 0.6883589029312134), ('dog', 0.6721124649047852), ('idea', 0.6635181307792664)]\n",
      "\n",
      "\n",
      "South Park model: \n",
      "\n",
      "[('woman', 0.8135941624641418), ('dog', 0.7553272843360901), ('boy', 0.7469039559364319), ('pot-bellied', 0.7415553331375122), ('lady', 0.732714056968689), ('man', 0.7274883985519409), ('penis', 0.7227966785430908), ('baby', 0.7196555137634277), ('pig', 0.7001212239265442), ('turd', 0.6882596015930176)]\n"
     ]
    }
   ],
   "source": [
    "def print_similar_words(word):\n",
    "    print(\"Similar words for the word: \" + word + \"\\n\")\n",
    "    print(\"Futurama model: \\n\" + \"\")\n",
    "    print(futur_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"American Dad model: \\n\")\n",
    "    print(amDad_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"Family Guy model: \\n\")\n",
    "    print(famGuy_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"Simpsons model: \\n\")\n",
    "    print(simp_model.wv.most_similar(word, topn=10))\n",
    "    print(\"\\n\")\n",
    "    print(\"South Park model: \\n\")\n",
    "    print(sp_model.wv.most_similar(word, topn=10))\n",
    "print_similar_words(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the word \"science\" also has interesting results. *South Park*'s similar words to \"science\" are \"Christian\" and \"comedy\". After seeing these words, I watched the *South Park* episode \"Go God Go\" where \"science\" and religion are the core themes. Throughout the episode, Richard Dawkins and Ms. Garrison attempt to rid the world of religious ideology in favor of scientific belief. In fact, the entire episode's plot centers upon the ways that scientific dogma can create factionalism similar to religious factionalism. This oppositional relationship between \"science\" and religion is not mirrored in other TV shows in the dataset where \"science\" is similar to \"progress\" in the *Family Guy* model and \"modern\" or \"mysterious\" in *The Simpsons* model. This relationship between religion and science is what differentiates episodes of *South Park* that investigate science, compared to thematically similar episodes of *The Simpsons*. For example, the episode \"The Monkey Suit\" involves themes of both religion and science beginning with a debate around creationism and evolution. However, this dichotomy between creationism and evolution is merely the set up for the plot rather than the central focus. The plot veers away and centers upon a trial against Lisa and an investigation that decides Homer is the evolutionary missing link. Despite the fact that science and religion begin the episode in opposition, there are no episodes where investigating this perceived dichotomy is the central focus of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for the word: science\n",
      "\n",
      "Futurama model: \n",
      "\n",
      "[('animals', 0.9976933598518372), ('form', 0.9974814057350159), ('straight', 0.9962860941886902), ('dignity', 0.9955741763114929), ('eternity', 0.9955719709396362), ('destroyed', 0.995414137840271), ('parts', 0.9952705502510071), ('sleeping', 0.9948841333389282), ('episode', 0.9948302507400513), ('tail', 0.9948232173919678)]\n",
      "\n",
      "\n",
      "American Dad model: \n",
      "\n",
      "[('post', 0.9672064185142517), ('points', 0.9643006324768066), ('patio', 0.9625817537307739), ('finest', 0.959784984588623), ('legal', 0.9576351642608643), ('market', 0.956371009349823), ('trees', 0.9562504291534424), ('replaced', 0.9558205604553223), ('backup', 0.9555937051773071), ('uniform', 0.955528736114502)]\n",
      "\n",
      "\n",
      "Family Guy model: \n",
      "\n",
      "[('custody', 0.9071251153945923), ('certificate', 0.9017797112464905), ('chevy', 0.8972327709197998), ('grade', 0.8955509662628174), ('profits', 0.8922510743141174), ('tradition', 0.8919973373413086), ('ticking', 0.891424298286438), ('elsewhere', 0.8908125162124634), ('scissors', 0.8903434872627258), ('undefeated', 0.8882365822792053)]\n",
      "\n",
      "\n",
      "Simpsons model: \n",
      "\n",
      "[('lowest', 0.8994623422622681), ('labor', 0.8918164968490601), ('rule', 0.8837964534759521), ('moments', 0.8768786191940308), ('award', 0.872724711894989), ('restored', 0.872229814529419), ('roosevelt', 0.8682748079299927), ('defense', 0.8650045990943909), ('murder', 0.8627876043319702), ('football', 0.8626381158828735)]\n",
      "\n",
      "\n",
      "South Park model: \n",
      "\n",
      "[('ed', 0.833633303642273), ('adventure', 0.7984679341316223), ('summer', 0.7934126853942871), ('greatest', 0.7924792766571045), ('witch', 0.790005624294281), ('awards', 0.7846487760543823), ('cause', 0.7828356027603149), ('christian', 0.7808429002761841), ('child', 0.7760815620422363), ('fair', 0.7751200795173645)]\n"
     ]
    }
   ],
   "source": [
    "print_similar_words(\"science\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the words \"dad\" and \"family\" are both highly weighted in the logistic regression for both the *Futurama*-*The Simpsons* and the *South Park*-*The Simpsons* classification models. Because of this high weighting, I decided to investigate the similar words to both \"dad\" and \"family\" in the word2vec models of all five shows in the dataset. Interestingly, every show except *Futurama* has nearly identical similar words for \"dad\" and \"family\". These similar words include \"life\", \"marriage\", \"mother\", \"child\" or \"friends\". However, *Futurama* breaks from the pack, possessing similar words like \"store\", \"date\", \"liquor\", \"crew\" and \"class\". There is again computational evidence for Matt Groening's and Matt Cohen's different writing in *Futurama* versus *The Simpsons*, but in this case, there is computational evidence for the vocabulary and themes in *Futurama* breaking from the genre of animated adult comedy TV shows as a whole. Almost every show in the genre is inundated with familial themes with a show that breaks from these themes being an exception rather than the rule. Prior to fully googling a list of shows in the genre, my dad, my brother and I thought of popular animated adult comedy TV shows that do not center around a familial unit or do not use a familial relationship to move the plot of many episodes across the length of the show's run. The only examples we could think of were *Futurama* and *BoJack Horseman*. I know many episodes of season 4 of *BoJack Horseman* center around BoJack's mother and his childhood, but compared to shows like *Archer*, the familial themes of the show are much more sporadic. Of course, this is anecdotal evidence based on three men with similar TV viewing habits, but it is nevertheless interesting that *Futurama* breaks from this convention despite Matt Groening having success with a family-based show in *The Simpsons*. After researching this further, I found an interesting [analysis](https://justtv.files.wordpress.com/2007/03/mittell_simpsons.pdf) written by Jason Mittell titled \"Cartoon Realism: Genre Mixing and the Cultural Life of *The Simpsons*\". In this paper, Mittell argues that using a familial unit as the basis for a TV comedy is relatively generic and uncontroversial in nature. This enables the TV show to be more successful especially on network television because of its much wider appeal. I agree with this analysis, but I do think that the emergence of streaming services allows shows with less of a broad appeal to see success. Mittell's article was written in 2007, so of course, he could not predict this trend, but perhaps, in the coming decades, the structure of a familial sitcom will become less of a norm in animated adult comedy. The deeper investigation that word2vec enabled highlights the usefulness of word2vec in enabling close reading and analysis of themes that may be overlooked when watching a show. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words for the word: family\n",
      "\n",
      "Futurama model: \n",
      "\n",
      "[('against', 0.9983259439468384), ('whose', 0.9980803728103638), ('wherever', 0.9980451464653015), ('warming', 0.9979935884475708), ('cause', 0.9979621767997742), ('large', 0.9979024529457092), ('smaller', 0.9977759122848511), ('judge', 0.9977691173553467), ('mutant', 0.9977593421936035), ('eyes', 0.9977487325668335)]\n",
      "\n",
      "\n",
      "American Dad model: \n",
      "\n",
      "[('house', 0.816673219203949), ('life', 0.8013492226600647), ('friends', 0.7967162132263184), ('party', 0.7960283756256104), ('chance', 0.7941237688064575), ('husband', 0.7878105640411377), ('child', 0.7807456254959106), ('room', 0.7767513990402222), ('wife', 0.7750695943832397), ('money', 0.7724897861480713)]\n",
      "\n",
      "\n",
      "Family Guy model: \n",
      "\n",
      "[('child', 0.5585479736328125), ('story', 0.5509899854660034), ('dog', 0.5475580096244812), ('new', 0.5388461947441101), ('body', 0.5280017852783203), ('book', 0.5255934596061707), ('girl', 0.5255507826805115), ('star', 0.5248533487319946), ('guy', 0.5237522721290588), ('friendship', 0.5153549909591675)]\n",
      "\n",
      "\n",
      "Simpsons model: \n",
      "\n",
      "[('marriage', 0.815654456615448), ('life', 0.7769157290458679), ('moment', 0.7289344072341919), ('father', 0.7287483215332031), ('honor', 0.7256622314453125), ('husband', 0.7244408130645752), ('mother', 0.7189117670059204), ('daughter', 0.7160970568656921), ('friend', 0.7139638662338257), ('wedding', 0.7120558023452759)]\n",
      "\n",
      "\n",
      "South Park model: \n",
      "\n",
      "[('child', 0.7645646333694458), ('soul', 0.749803900718689), ('mother', 0.7488617300987244), ('life', 0.7439771890640259), ('lives', 0.7212262749671936), ('friend', 0.7180187702178955), ('future', 0.7137486934661865), ('fault', 0.7120407819747925), ('brother', 0.7085095643997192), ('tummy', 0.6941323280334473)]\n"
     ]
    }
   ],
   "source": [
    "print_similar_words(\"family\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project adds to the ongoing discussion about adult animated comedy shows, hoping to provide answers about the distinctions between the most popular shows in the genre over the last three decades. However, there are limitations to the analysis presented. First, as I discussed in the data section above, there are a few shows including *Archer* and *King of the Hill* that are not included in the dataset due to copyright issues filed by the shows' production studios. Second, if a show is currently still releasing new episodes, the most recent season of a TV show does not have available transcripts. For example, \"The Pandemic Special\" of *South Park* does not have an available transcript because it was released recently. Additionally, as stated in the \"Relevant Studies\" section above, TV show genres and their writing are constantly changing, so many conclusions could be altered or even reversed as consumer preferences change or streaming services become more important to the production of television shows. \n",
    "\n",
    "Going forward, analyzing the results of Matt Groening's and Seth McFarland's writing in different shows was intriguing, and I would like to see how the same comedy writers change over time or write differently when writing for two shows that are running at the same time. Additionally, similar analyses could be done for other TV show genres or subsections of TV show genres. For example, analysis could be done on the difference in writing between multi-cam sitcoms such as *Big Bang Theory* and *2 Broke Girls* and single-cam sitcoms such as *Modern Family* and *Silicon Valley*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
